{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474574eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pandas scikit-learn torch torchvision sentence-transformers timm tqdm Pillow requests psutil\n",
    "\n",
    "# This command installs the specific CLIP library from its GitHub repository\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "print(\"\\nAll libraries installed or updated.\")\n",
    "print(\"IMPORTANT: Please RESTART THE KERNEL now for the changes to take effect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3349e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "# ML Libraries\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "from src.utils import download_images # Make sure utils.py is in src/ folder\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TRAIN_IMAGE_DIR = 'test_images'  # Folder with TRAIN photos\n",
    "TEST_IMAGE_DIR = 'train_images'   # Folder with TEST photos\n",
    "\n",
    "\n",
    "# Device Selection for Mac M1 GPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Setup Complete. Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Train data shape: {df_train.shape}\")\n",
    "print(f\"Test data shape: {df_test.shape}\")\n",
    "\n",
    "print(\"\\nTrain data sample:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for FAST and EFFICIENT Downloading\n",
    "\n",
    "# Import the main download function from your updated utils.py\n",
    "from src.utils import download_images\n",
    "from pathlib import Path\n",
    "\n",
    "def run_smart_download(df, img_dir):\n",
    "    \"\"\"\n",
    "    Checks for missing files and calls the official download script only for them.\n",
    "    \"\"\"\n",
    "    img_dir_path = Path(img_dir)\n",
    "    print(f\"--- Verifying images in '{img_dir_path.name}' ---\")\n",
    "\n",
    "    expected_ids = set(df['sample_id'].astype(str))\n",
    "    existing_ids = {f.stem for f in img_dir_path.glob('*.jpg')}\n",
    "    missing_ids = expected_ids - existing_ids\n",
    "    \n",
    "    print(f\"Found {len(existing_ids)} of {len(expected_ids)} expected images.\")\n",
    "\n",
    "    if not missing_ids:\n",
    "        print(\"All images are present. No download needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{len(missing_ids)} missing image(s) detected. Preparing to download.\")\n",
    "    \n",
    "    # Filter the DataFrame to get only the rows for the missing images\n",
    "    df_missing = df[df['sample_id'].astype(str).isin(missing_ids)]\n",
    "    \n",
    "    # Create the list of tasks [(sample_id, image_link), ...]\n",
    "    tasks_to_run = list(zip(df_missing['sample_id'], df_missing['image_link']))\n",
    "    \n",
    "    # Call the download function from utils.py\n",
    "    download_images(tasks_to_run, str(img_dir_path))\n",
    "    \n",
    "    print(\"\\nDownload attempt complete.\")\n",
    "\n",
    "# Execute the download for both sets.\n",
    "# Make sure TRAIN_IMAGE_DIR and TEST_IMAGE_DIR are set correctly from your setup cell!\n",
    "run_smart_download(df_train, TRAIN_IMAGE_DIR)\n",
    "print(\"-\" * 30)\n",
    "run_smart_download(df_test, TEST_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a7b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path # <-- IMPORT THE PATH OBJECT\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to Create Directories\n",
    "def create_directories(*dir_paths):\n",
    "    \"\"\"Creates one or more directories if they do not already exist.\"\"\"\n",
    "    print(\"Ensuring image directories exist...\")\n",
    "    for path in dir_paths:\n",
    "        # The path variable is now a proper Path object\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"All necessary directories are created or already exist.\")\n",
    "\n",
    "# Setup\n",
    "# Define base data directory and load dataframes\n",
    "BASE_PATH = Path('./data')\n",
    "\n",
    "# THE FIX: Use Path() to define your directories \n",
    "TRAIN_IMAGE_DIR = BASE_PATH / 'test_images'\n",
    "TEST_IMAGE_DIR = BASE_PATH / 'train_images'\n",
    "\n",
    "# Call the function to create the directories\n",
    "create_directories(TRAIN_IMAGE_DIR, TEST_IMAGE_DIR)\n",
    "\n",
    "# Load DataFrames\n",
    "# Assuming df_test is loaded from a CSV file in the 'data' folder\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Clean the Test Images Folder\n",
    "print(f\"\\nCleaning the test images folder: {TEST_IMAGE_DIR}\")\n",
    "\n",
    "# Create a set of valid sample IDs for the test set for fast lookups\n",
    "valid_test_ids = set(df_test['sample_id'].astype(str))\n",
    "\n",
    "files_to_delete = []\n",
    "# Find all files that do NOT belong in the test set\n",
    "for f_path in TEST_IMAGE_DIR.glob('*.jpg'):\n",
    "    if f_path.stem not in valid_test_ids:\n",
    "        files_to_delete.append(f_path)\n",
    "\n",
    "if not files_to_delete:\n",
    "    print(\"No extra files found. Folder is already clean!\")\n",
    "else:\n",
    "    print(f\"Found {len(files_to_delete)} extra files to delete. Deleting now...\")\n",
    "    for f in tqdm(files_to_delete, desc=\"Cleaning\"):\n",
    "        f.unlink() # This deletes the file\n",
    "    print(\"Cleaning complete.\")\n",
    "\n",
    "# Final Verification\n",
    "train_count = len(list(TRAIN_IMAGE_DIR.glob('*.jpg')))\n",
    "test_count = len(list(TEST_IMAGE_DIR.glob('*.jpg')))\n",
    "\n",
    "print(\"\\nFinal File Counts\")\n",
    "print(f\"Images in TRAIN folder ({TRAIN_IMAGE_DIR.name}): {train_count}\")\n",
    "print(f\"Images in TEST folder ({TEST_IMAGE_DIR.name}):  {test_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Parse Catalog Content\n",
    "import re\n",
    "\n",
    "def parse_content(content_string):\n",
    "    \"\"\"\n",
    "    Parses the raw catalog_content string into separate, clean features.\n",
    "    \"\"\"\n",
    "    if not isinstance(content_string, str):\n",
    "        content_string = \"\"\n",
    "        \n",
    "    lines = content_string.strip().split('\\n')\n",
    "    \n",
    "    # Default values\n",
    "    item_name = \"\"\n",
    "    bullet_points = []\n",
    "    prod_desc = \"\"\n",
    "    value = 1.0  # Default to 1 if not found\n",
    "    unit = \"Unknown\"\n",
    "\n",
    "    for line in lines:\n",
    "        if line.lower().startswith(\"item name:\"):\n",
    "            item_name = line[len(\"item name:\"):].strip()\n",
    "        elif line.lower().startswith(\"bullet point\"):\n",
    "            bp_text = re.sub(r'Bullet Point \\d+:', '', line, flags=re.IGNORECASE).strip()\n",
    "            bullet_points.append(bp_text)\n",
    "        elif line.lower().startswith(\"product description:\"):\n",
    "            prod_desc = line[len(\"product description:\"):].strip()\n",
    "        elif line.lower().startswith(\"value:\"):\n",
    "            try:\n",
    "                value = float(line[len(\"value:\"):].strip())\n",
    "            except (ValueError, TypeError):\n",
    "                value = 1.0 # Keep default if parsing fails\n",
    "        elif line.lower().startswith(\"unit:\"):\n",
    "            unit = line[len(\"unit:\"):].strip()\n",
    "            \n",
    "    # Combine all text fields into a single 'clean_text' feature\n",
    "    clean_text = \" \".join([item_name] + bullet_points + [prod_desc]).strip()\n",
    "    \n",
    "    return pd.Series([clean_text, value, unit], index=['clean_text', 'quantity', 'unit'])\n",
    "\n",
    "# --- Apply the function to both train and test dataframes ---\n",
    "print(\"Parsing training data...\")\n",
    "df_train_parsed = df_train['catalog_content'].apply(parse_content)\n",
    "df_train = pd.concat([df_train, df_train_parsed], axis=1)\n",
    "\n",
    "print(\"Parsing test data...\")\n",
    "df_test_parsed = df_test['catalog_content'].apply(parse_content)\n",
    "df_test = pd.concat([df_test, df_test_parsed], axis=1)\n",
    "\n",
    "# Display the new columns to verify\n",
    "print(\"\\nNew features created successfully!\")\n",
    "df_train[['clean_text', 'quantity', 'unit']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3d873a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Setting up environment\n",
      "Using device: mps\n",
      "\n",
      "Step 2: Loading BALANCED CLIP model\n",
      "CLIP model loaded.\n",
      "\n",
      "--- Generating 'train' Image Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train batches:   0%|          | 0/1172 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved feature array to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAVE_DIR_MID\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_image_embeds_full.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# --- 5. EXECUTE THE PROCESS ---\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mgenerate_image_embeddings_stable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_IMAGE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m generate_image_embeddings_stable(df_test, TEST_IMAGE_DIR, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Final cleanup\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 65\u001b[0m, in \u001b[0;36mgenerate_image_embeddings_stable\u001b[0;34m(df, image_dir, prefix)\u001b[0m\n\u001b[1;32m     63\u001b[0m image_batch \u001b[38;5;241m=\u001b[39m image_batch\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Generate embeddings and move them to CPU immediately to save GPU memory\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m embeds \u001b[38;5;241m=\u001b[39m \u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     66\u001b[0m all_embeds\u001b[38;5;241m.\u001b[39mappend(embeds)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Aggressively clear memory after each batch\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GENERATE UPGRADED \"HD\" IMAGE EMBEDDINGS (STABLE MODE)\n",
    "import ssl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "from PIL import Image\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. SETUP\n",
    "print(\"\\nStep 1: Setting up environment\")\n",
    "DATA_DIR = Path('./data')\n",
    "# Your folders were swapped, so we point to the correct locations\n",
    "TRAIN_IMAGE_DIR = DATA_DIR / 'test_images'\n",
    "TEST_IMAGE_DIR = DATA_DIR / 'train_images'\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "SAVE_DIR_MID = Path(\"embeddings_medium\"); SAVE_DIR_MID.mkdir(exist_ok=True)\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 2. LOAD THE UPGRADED MODEL\n",
    "print(\"\\nStep 2: Loading BALANCED CLIP model\")\n",
    "# This is a powerful model that is stable on 8GB RAM systems.\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/16\", device=DEVICE) \n",
    "print(\"CLIP model loaded.\")\n",
    "\n",
    "# 3. DEFINE THE DATA LOADER\n",
    "# This class loads images one by one for the DataLoader.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            return self.transform(img)\n",
    "        except (FileNotFoundError, OSError):\n",
    "            # Return a blank placeholder if an image is missing\n",
    "            return torch.zeros(3, 224, 224)\n",
    "\n",
    "# 4. GENERATION FUNCTION\n",
    "def generate_image_embeddings_stable(df, image_dir, prefix):\n",
    "    print(f\"\\n--- Generating '{prefix}' Image Embeddings\")\n",
    "    image_paths = [Path(image_dir) / f\"{sid}.jpg\" for sid in df['sample_id'].astype(str)]\n",
    "    \n",
    "    dataset = ImageDataset(image_paths, clip_preprocess)\n",
    "    # num_workers=0 is crucial for stability on macOS in a notebook\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    \n",
    "    all_embeds = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch in tqdm(dataloader, desc=f\"Processing {prefix} batches\"):\n",
    "            image_batch = image_batch.to(DEVICE)\n",
    "            # Generate embeddings and move them to CPU immediately to save GPU memory\n",
    "            embeds = clip_model.encode_image(image_batch).cpu().numpy()\n",
    "            all_embeds.append(embeds)\n",
    "            \n",
    "            # Aggressively clear memory after each batch\n",
    "            del image_batch, embeds\n",
    "            if DEVICE == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # Combine all batch results into one final array\n",
    "    full_embeddings = np.vstack(all_embeds)\n",
    "    # Save the complete array to a single file\n",
    "    np.save(SAVE_DIR_MID / f\"{prefix}_image_embeds_full.npy\", full_embeddings)\n",
    "    print(f\"Saved feature array to: {SAVE_DIR_MID / f'{prefix}_image_embeds_full.npy'}\")\n",
    "\n",
    "# --- 5. EXECUTE THE PROCESS ---\n",
    "generate_image_embeddings_stable(df_train, TRAIN_IMAGE_DIR, prefix=\"train\")\n",
    "generate_image_embeddings_stable(df_test, TEST_IMAGE_DIR, prefix=\"test\")\n",
    "\n",
    "# Final cleanup\n",
    "del clip_model, df_train, df_test\n",
    "gc.collect()\n",
    "if DEVICE == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "print(\"\\nUpgraded 'HD' image embeddings have been generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Generate Text Embeddings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"--- Step 1 of 2: Generating Text Embeddings ---\")\n",
    "\n",
    "# --- Setup ---\n",
    "SAVE_DIR = Path(\"embeddings\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# This re-runs the parsing logic to get the 'clean_text' column\n",
    "import re\n",
    "BRAND_LIST = [\n",
    "    'nescafe', 'starbucks', 'keurig', 'dunkin', 'lavazza', 'peet\\'s', 'folgers', 'tassimo',\n",
    "    'samsung', 'apple', 'sony', 'lg', 'panasonic', 'bose', 'dell', 'hp', 'lenovo', 'acer', 'microsoft',\n",
    "    'nike', 'adidas', 'under armour', 'puma', 'reebok', 'new balance', 'champion',\n",
    "    'lego', 'hasbro', 'mattel', 'nerf', 'funko', 'play-doh',\n",
    "    'amazonbasics', 'kirkland signature', 'great value', 'up&up', 'logitech'\n",
    "]\n",
    "def parse_content_with_brand(content_string):\n",
    "    if not isinstance(content_string, str): content_string = \"\"\n",
    "    lines = content_string.strip().split('\\n'); item_name, bullet_points, value, unit, brand = \"\", [], 1.0, \"Unknown\", \"Unknown\"\n",
    "    for line in lines:\n",
    "        if line.lower().startswith(\"item name:\"): item_name = line[len(\"item name:\"):].strip()\n",
    "        elif line.lower().startswith(\"bullet point\"): bullet_points.append(re.sub(r'Bullet Point \\d+:', '', line, flags=re.IGNORECASE).strip())\n",
    "        elif line.lower().startswith(\"value:\"):\n",
    "            try: value = float(line[len(\"value:\"):].strip())\n",
    "            except: value = 1.0\n",
    "        elif line.lower().startswith(\"unit:\"): unit = line[len(\"unit:\"):].strip()\n",
    "    clean_text = \" \".join([item_name] + bullet_points).strip()\n",
    "    text_for_brand_check = (item_name + \" \" + (bullet_points[0] if bullet_points else \"\")).lower()\n",
    "    for b in BRAND_LIST:\n",
    "        if f' {b} ' in f' {text_for_brand_check} ': brand = b; break\n",
    "    return pd.Series([clean_text, value, unit, brand], index=['clean_text', 'quantity', 'unit', 'brand'])\n",
    "\n",
    "df_train = pd.concat([df_train, df_train['catalog_content'].apply(parse_content_with_brand)], axis=1)\n",
    "df_test = pd.concat([df_test, df_test['catalog_content'].apply(parse_content_with_brand)], axis=1)\n",
    "\n",
    "\n",
    "# Load Model \n",
    "print(\"\\nLoading text model...\")\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
    "\n",
    "# Generation Function\n",
    "def generate_text_embeddings(df, column, prefix):\n",
    "    print(f\"\\nGenerating Text Embeddings for '{prefix}' set\")\n",
    "    texts = df[column].tolist()\n",
    "    EMB_CHUNK = 10000 # Process in chunks to be memory safe\n",
    "\n",
    "    for start in tqdm(range(0, len(texts), EMB_CHUNK), desc=f\"Processing {prefix} chunks\"):\n",
    "        end = min(start + EMB_CHUNK, len(texts))\n",
    "        batch_texts = texts[start:end]\n",
    "        \n",
    "        embeds = text_model.encode(\n",
    "            batch_texts, \n",
    "            batch_size=128, \n",
    "            show_progress_bar=True, \n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        np.save(SAVE_DIR / f\"{prefix}_text_embeds_{start}_{end}.npy\", embeds)\n",
    "\n",
    "# Execute\n",
    "generate_text_embeddings(df_train, \"clean_text\", prefix=\"train\")\n",
    "generate_text_embeddings(df_test, \"clean_text\", prefix=\"test\")\n",
    "\n",
    "del text_model; gc.collect()\n",
    "if DEVICE == \"mps\": torch.mps.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Text embeddings have been generated successfully!\")\n",
    "print(f\"   Files are saved in the '{SAVE_DIR}' folder.\")\n",
    "print(\"   You can now re-run the 'Build Final Dataset' cell.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Build Final Datasets from All Features\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "print(\"\\nStep 1: Loading all data and feature references\")\n",
    "\n",
    "# Define paths to your data and embedding folders\n",
    "SAVE_DIR_TEXT = Path(\"data\")\n",
    "SAVE_DIR_IMAGE = Path(\"embeddings_medium\")\n",
    "\n",
    "# Load dataframes to get quantity, unit, and brand\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# --- This re-runs the parsing logic to ensure the brand/unit columns are available ---\n",
    "# (It's very fast and safe to re-run)\n",
    "BRAND_LIST = [\n",
    "    'nescafe', 'starbucks', 'keurig', 'dunkin', 'lavazza', 'peet\\'s', 'folgers', 'tassimo',\n",
    "    'samsung', 'apple', 'sony', 'lg', 'panasonic', 'bose', 'dell', 'hp', 'lenovo', 'acer', 'microsoft',\n",
    "    'nike', 'adidas', 'under armour', 'puma', 'reebok', 'new balance', 'champion',\n",
    "    'lego', 'hasbro', 'mattel', 'nerf', 'funko', 'play-doh',\n",
    "    'amazonbasics', 'kirkland signature', 'great value', 'up&up', 'logitech'\n",
    "]\n",
    "import re\n",
    "def parse_content_with_brand(content_string):\n",
    "    if not isinstance(content_string, str): content_string = \"\"\n",
    "    lines = content_string.strip().split('\\n'); item_name, bullet_points, value, unit, brand = \"\", [], 1.0, \"Unknown\", \"Unknown\"\n",
    "    for line in lines:\n",
    "        if line.lower().startswith(\"item name:\"): item_name = line[len(\"item name:\"):].strip()\n",
    "        elif line.lower().startswith(\"bullet point\"): bullet_points.append(re.sub(r'Bullet Point \\d+:', '', line, flags=re.IGNORECASE).strip())\n",
    "        elif line.lower().startswith(\"value:\"):\n",
    "            try: value = float(line[len(\"value:\"):].strip())\n",
    "            except: value = 1.0\n",
    "        elif line.lower().startswith(\"unit:\"): unit = line[len(\"unit:\"):].strip()\n",
    "    clean_text = \" \".join([item_name] + bullet_points).strip()\n",
    "    text_for_brand_check = (item_name + \" \" + (bullet_points[0] if bullet_points else \"\")).lower()\n",
    "    for b in BRAND_LIST:\n",
    "        if f' {b} ' in f' {text_for_brand_check} ': brand = b; break\n",
    "    return pd.Series([clean_text, value, unit, brand], index=['clean_text', 'quantity', 'unit', 'brand'])\n",
    "\n",
    "df_train = pd.concat([df_train, df_train['catalog_content'].apply(parse_content_with_brand)], axis=1)\n",
    "df_test = pd.concat([df_test, df_test['catalog_content'].apply(parse_content_with_brand)], axis=1)\n",
    "print(\"Dataframes with brand/quantity features are ready.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Prepare categorical features ---\n",
    "print(\"\\nStep 2: Preparing categorical features (unit and brand)\")\n",
    "train_cats = pd.get_dummies(df_train[['unit', 'brand']], prefix=['unit', 'brand'])\n",
    "test_cats = pd.get_dummies(df_test[['unit', 'brand']], prefix=['unit', 'brand'])\n",
    "train_cats_aligned, test_cats_aligned = train_cats.align(test_cats, join='outer', axis=1, fill_value=0)\n",
    "print(\"Categorical features prepared.\")\n",
    "\n",
    "\n",
    "# Step 3: Combine and save the final arrays\n",
    "def build_final_dataset(prefix, df, cat_features):\n",
    "    print(f\"\\nBuilding final dataset for '{prefix}'\")\n",
    "    \n",
    "    # Load all text embedding chunks and combine them\n",
    "    text_files = sorted(SAVE_DIR_TEXT.glob(f\"{prefix}_text_embeds_*.npy\"))\n",
    "    if not text_files: raise FileNotFoundError(f\"No text embedding files found for '{prefix}'.\")\n",
    "    text_embeds = np.vstack([np.load(f) for f in text_files])\n",
    "    \n",
    "    # Load the single, complete image embedding file\n",
    "    image_embeds = np.load(SAVE_DIR_IMAGE / f\"{prefix}_image_embeds_full.npy\")\n",
    "    \n",
    "    # Get the quantity column and fill any missing values\n",
    "    quantity = df['quantity'].fillna(1.0).values.reshape(-1, 1)\n",
    "    \n",
    "    # Get the one-hot encoded categorical features\n",
    "    cats = cat_features.values\n",
    "    \n",
    "    # This is the key step: stack everything side-by-side\n",
    "    final_X = np.hstack([\n",
    "        text_embeds.astype(np.float32),      # Text features\n",
    "        image_embeds.astype(np.float32),     # Image features\n",
    "        quantity.astype(np.float32),         # Quantity feature\n",
    "        cats.astype(np.float32)              # Brand and Unit features\n",
    "    ])\n",
    "    \n",
    "    # Save the final, combined array\n",
    "    save_path = SAVE_DIR_IMAGE / f\"final_X_{prefix}_medium_with_brand.npy\"\n",
    "    np.save(save_path, final_X)\n",
    "    print(f\"Saved final feature array to: {save_path} with shape {final_X.shape}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del text_embeds, image_embeds, quantity, cats, final_X; gc.collect()\n",
    "\n",
    "# Execute for both train and test sets --\n",
    "build_final_dataset(\"train\", df_train, train_cats_aligned)\n",
    "build_final_dataset(\"test\", df_test, test_cats_aligned)\n",
    "\n",
    "print(\"\\nAll final feature arrays have been created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeedb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED MLP FUSION FOR TEXT + IMAGE EMBEDDINGS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MLP FUSION: Text + Image Embeddings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ADVANCED MLP ARCHITECTURE\n",
    "\n",
    "class MultimodalFusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced fusion with separate encoders and attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim, image_dim, other_dim, hidden_dim=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Linear(text_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Linear(image_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.other_encoder = nn.Sequential( # Encoder for quantity, brand, etc.\n",
    "            nn.Linear(other_dim, 64), nn.LayerNorm(64), nn.ReLU(), nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, dropout=0.1, batch_first=True)\n",
    "        \n",
    "        # Fusion layers to combine all encoded parts\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + 64, hidden_dim * 2), nn.LayerNorm(hidden_dim * 2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(), nn.Dropout(dropout * 0.7),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), nn.LayerNorm(hidden_dim // 2), nn.ReLU(), nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, text_emb, image_emb, other_emb):\n",
    "        text_enc = self.text_encoder(text_emb)\n",
    "        image_enc = self.image_encoder(image_emb)\n",
    "        other_enc = self.other_encoder(other_emb)\n",
    "        \n",
    "        # Cross-attention: text attends to image\n",
    "        attended, _ = self.attention(text_enc.unsqueeze(1), image_enc.unsqueeze(1), image_enc.unsqueeze(1))\n",
    "        attended = attended.squeeze(1)\n",
    "        \n",
    "        # Concatenate attended text, original image, and other features\n",
    "        fused = torch.cat([attended, image_enc, other_enc], dim=1)\n",
    "        output = self.fusion(fused)\n",
    "        return output\n",
    "\n",
    "# TRAINING FUNCTION (UPDATED FOR 3 INPUTS)\n",
    "\n",
    "def train_mlp_fusion(X_text_tr, X_image_tr, X_other_tr, y_tr, \n",
    "                     X_text_val, X_image_val, X_other_val, y_val,\n",
    "                     epochs=100, batch_size=256, lr=5e-4):\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print(f\"  Using device: {device}\")\n",
    "    \n",
    "    text_dim, image_dim, other_dim = X_text_tr.shape[1], X_image_tr.shape[1], X_other_tr.shape[1]\n",
    "    \n",
    "    model = MultimodalFusionMLP(text_dim, image_dim, other_dim).to(device)\n",
    "    \n",
    "    def pseudo_huber_loss(pred, target, delta=1.0):\n",
    "        residual = pred - target\n",
    "        return torch.mean(delta**2 * (torch.sqrt(1 + (residual/delta)**2) - 1))\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_text_tr), torch.FloatTensor(X_image_tr), torch.FloatTensor(X_other_tr), torch.FloatTensor(y_tr).unsqueeze(1))\n",
    "    val_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_text_val), torch.FloatTensor(X_image_val), torch.FloatTensor(X_other_val), torch.FloatTensor(y_val).unsqueeze(1))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    best_val_loss = float('inf'); patience, patience_counter = 15, 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train(); train_loss = 0\n",
    "        for text_b, image_b, other_b, y_b in train_loader:\n",
    "            text_b, image_b, other_b, y_b = text_b.to(device), image_b.to(device), other_b.to(device), y_b.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text_b, image_b, other_b)\n",
    "            loss = pseudo_huber_loss(output, y_b)\n",
    "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval(); val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for text_b, image_b, other_b, y_b in val_loader:\n",
    "                text_b, image_b, other_b, y_b = text_b.to(device), image_b.to(device), other_b.to(device), y_b.to(device)\n",
    "                output = model(text_b, image_b, other_b)\n",
    "                val_loss += pseudo_huber_loss(output, y_b).item()\n",
    "        \n",
    "        train_loss /= len(train_loader); val_loss /= len(val_loader); scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss, best_model_state, patience_counter = val_loss, model.state_dict(), 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience: print(f\"    Early stopping at epoch {epoch+1}\"); break\n",
    "        if (epoch + 1) % 10 == 0: print(f\"    Epoch {epoch+1}: train_loss={train_loss:.5f}, val_loss={val_loss:.5f}\")\n",
    "            \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# PREDICTION FUNCTION (UPDATED FOR 3 INPUTS)\n",
    "\n",
    "def predict_mlp(model, X_text, X_image, X_other, batch_size=256):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval(); predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(X_text), batch_size), desc=\"Predicting\", leave=False):\n",
    "            end_idx = min(i + batch_size, len(X_text))\n",
    "            text_b = torch.FloatTensor(X_text[i:end_idx]).to(device)\n",
    "            image_b = torch.FloatTensor(X_image[i:end_idx]).to(device)\n",
    "            other_b = torch.FloatTensor(X_other[i:end_idx]).to(device)\n",
    "            output = model(text_b, image_b, other_b)\n",
    "            predictions.append(output.cpu().numpy())\n",
    "    return np.vstack(predictions).flatten()\n",
    "\n",
    "# CORRECTED DATA LOADING AND SLICING \n",
    "print(\"\\n[1/4] Loading and slicing combined embeddings...\")\n",
    "df_train = pd.read_csv('train.csv')\n",
    "y_train_log = np.log1p(df_train['price'].values)\n",
    "\n",
    "# Load the SINGLE, COMBINED feature files , use correct paths\n",
    "X_train_full = np.load(\"final_X_train_medium_with_brand.npy\", allow_pickle=False)\n",
    "X_test_full = np.load(\"final_X_test_medium_with_brand.npy\", allow_pickle=False)\n",
    "\n",
    "# Define the dimensions of your features\n",
    "text_dim = 384 # From SentenceTransformer\n",
    "image_dim = 512 # From ViT-B/16\n",
    "\n",
    "# Slice the combined arrays into their constituent parts\n",
    "train_text = X_train_full[:, :text_dim]\n",
    "train_image = X_train_full[:, text_dim:text_dim+image_dim]\n",
    "train_other = X_train_full[:, text_dim+image_dim:]\n",
    "\n",
    "test_text = X_test_full[:, :text_dim]\n",
    "test_image = X_test_full[:, text_dim:text_dim+image_dim]\n",
    "test_other = X_test_full[:, text_dim+image_dim:]\n",
    "\n",
    "print(f\"✓ Text: train{train_text.shape}, test{test_text.shape}\")\n",
    "print(f\"✓ Image: train{train_image.shape}, test{test_image.shape}\")\n",
    "print(f\"✓ Other: train{train_other.shape}, test{test_other.shape}\")\n",
    "del X_train_full, X_test_full; gc.collect()\n",
    "\n",
    "# SCALE FEATURES\n",
    "\n",
    "print(\"\\n[2/4] Scaling features...\")\n",
    "text_scaler, image_scaler, other_scaler = RobustScaler(), RobustScaler(), RobustScaler()\n",
    "\n",
    "train_text_scaled = text_scaler.fit_transform(train_text); test_text_scaled = text_scaler.transform(test_text)\n",
    "train_image_scaled = image_scaler.fit_transform(train_image); test_image_scaled = image_scaler.transform(test_image)\n",
    "train_other_scaled = other_scaler.fit_transform(train_other); test_other_scaled = other_scaler.transform(test_other)\n",
    "\n",
    "print(\"✓ Features scaled\")\n",
    "del train_text, train_image, train_other, test_text, test_image, test_other; gc.collect()\n",
    "\n",
    "# K-FOLD TRAINING\n",
    "\n",
    "print(\"\\n[3/4] Training MLP with K-Fold...\")\n",
    "N_FOLDS = 5; kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(train_text_scaled)); test_preds = np.zeros(len(test_text_scaled))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_text_scaled), 1):\n",
    "    print(f\"\\n{'─'*70}\\nFOLD {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    model = train_mlp_fusion(\n",
    "        train_text_scaled[train_idx], train_image_scaled[train_idx], train_other_scaled[train_idx], y_train_log[train_idx],\n",
    "        train_text_scaled[val_idx], train_image_scaled[val_idx], train_other_scaled[val_idx], y_train_log[val_idx]\n",
    "    )\n",
    "    \n",
    "    oof_preds[val_idx] = predict_mlp(model, train_text_scaled[val_idx], train_image_scaled[val_idx], train_other_scaled[val_idx])\n",
    "    test_preds += predict_mlp(model, test_text_scaled, test_image_scaled, test_other_scaled) / N_FOLDS\n",
    "    \n",
    "    val_pred_price = np.expm1(oof_preds[val_idx]); val_actual_price = np.expm1(y_train_log[val_idx])\n",
    "    fold_smape = np.mean(2 * np.abs(val_pred_price - val_actual_price) / (np.abs(val_actual_price) + np.abs(val_pred_price) + 1e-8)) * 100\n",
    "    print(f\"Fold {fold} SMAPE: {fold_smape:.4f}%\")\n",
    "    \n",
    "    del model; gc.collect(); torch.mps.empty_cache() if torch.backends.mps.is_available() else None\n",
    "\n",
    "\n",
    "# FINAL EVALUATION\n",
    "\n",
    "print(\"\\n[4/4] Final evaluation and submission...\")\n",
    "oof_prices = np.expm1(oof_preds); actual_prices = df_train['price'].values\n",
    "overall_smape = np.mean(2 * np.abs(oof_prices - actual_prices) / (np.abs(actual_prices) + np.abs(oof_prices) + 1e-8)) * 100\n",
    "print(\"\\n\" + \"=\"*70 + f\"\\nFINAL OOF SMAPE: {overall_smape:.4f}%\\n\" + \"=\"*70)\n",
    "\n",
    "final_predictions = np.expm1(test_preds); final_predictions = np.clip(final_predictions, 0.01, None)\n",
    "df_test = pd.read_csv('test.csv')\n",
    "submission = pd.DataFrame({'sample_id': df_test['sample_id'],'price': final_predictions})\n",
    "submission.to_csv('test_out.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission created: test_out.csv\")\n",
    "print(\"\\nFirst 10 predictions:\"); print(submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
