{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3349e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. Using device: mps\n",
      "\n",
      "Path Corrections\n",
      "TRAIN_IMAGE_DIR is now set to: data/test_images\n",
      "TEST_IMAGE_DIR is now set to:  data/train_images\n"
     ]
    }
   ],
   "source": [
    "# Python Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "# ML Libraries\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "from src.utils import download_images # Make sure utils.py is in src/ folder\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "DATA_DIR = Path('./data')\n",
    "\n",
    "TRAIN_IMAGE_DIR = DATA_DIR / 'test_images'  # Folder with TRAIN photos\n",
    "TEST_IMAGE_DIR = DATA_DIR / 'train_images'   # Folder with TEST photos\n",
    "\n",
    "# Confirm the directories exist\n",
    "TRAIN_IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device Selection for Mac M1 GPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Setup Complete. Using device: {DEVICE}\")\n",
    "print(\"\\nPath Corrections\")\n",
    "print(f\"TRAIN_IMAGE_DIR is now set to: {TRAIN_IMAGE_DIR}\")\n",
    "print(f\"TEST_IMAGE_DIR is now set to:  {TEST_IMAGE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f512e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (75000, 4)\n",
      "Test data shape: (75000, 3)\n",
      "\n",
      "Train data sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judeeâ€™s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judeeâ€™s Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  \n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "\n",
    "df_train = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "df_test = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "\n",
    "print(f\"Train data shape: {df_train.shape}\")\n",
    "print(f\"Test data shape: {df_test.shape}\")\n",
    "\n",
    "print(\"\\nTrain data sample:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718182be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths are now correctly set:\n",
      "Train images path: data/train_images\n",
      "Test images path: data/test_images\n"
     ]
    }
   ],
   "source": [
    "# Define paths that match your actual folder names\n",
    "TRAIN_IMG_DIR = DATA_DIR / 'train_images'\n",
    "TEST_IMG_DIR = DATA_DIR / 'test_images'\n",
    "\n",
    "# This will just confirm the folders exist and won't create new ones\n",
    "TRAIN_IMG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TEST_IMG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Paths are now correctly set:\")\n",
    "print(f\"Train images path: {TRAIN_IMG_DIR}\")\n",
    "print(f\"Test images path: {TEST_IMG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f3c179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in train_images: 74999\n",
      "Total files in test_images: 74999\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "#from PIL import Image\n",
    "#from io import BytesIO\n",
    "#from tqdm import tqdm\n",
    "#import os\n",
    "\n",
    "#def download_single_image(row, img_dir):\n",
    "#    \"\"\"Download a single image given a row with sample_id and image_link\"\"\"\n",
    "#    sample_id = row['sample_id']\n",
    "#    img_url = row['image_link']\n",
    "#    img_path = os.path.join(img_dir, f\"{sample_id}.jpg\")\n",
    "    \n",
    "    # Skip if already downloaded\n",
    "#    if os.path.exists(img_path):\n",
    "#        return True\n",
    "    \n",
    "#    try:\n",
    "#        response = requests.get(img_url, timeout=10)\n",
    " #       if response.status_code == 200:\n",
    "#            img = Image.open(BytesIO(response.content))\n",
    "#            img = img.convert('RGB')\n",
    "#            img.save(img_path, 'JPEG')\n",
    "#            return True\n",
    "#    except Exception as e:\n",
    "#        return False\n",
    "    \n",
    "#    return False\n",
    "\n",
    "#print(\"Downloading training images...\")\n",
    "#print(\"This will take a while. Progress will be shown below.\")\n",
    "\n",
    "# Download without multiprocessing (more stable on Mac)\n",
    "#successful = 0\n",
    "#failed = 0\n",
    "\n",
    "#for idx, row in tqdm(df_train.iterrows(), total=len(df_train), desc=\"Train images\"):\n",
    "#    if download_single_image(row, str(TRAIN_IMG_DIR)):\n",
    "#       successful += 1\n",
    "#    else:\n",
    "#        failed += 1\n",
    "    \n",
    "    # Optional: Add a small delay every 100 images to avoid throttling\n",
    "#   if idx % 100 == 0 and idx > 0:\n",
    "#      time.sleep(0.5)\n",
    "\n",
    "#print(f\"\\nTraining images download complete!\")\n",
    "#print(f\"Successful: {successful}, Failed: {failed}\")\n",
    "print(f\"Total files in train_images: {len(list(TRAIN_IMG_DIR.glob('*.jpg')))}\")\n",
    "\n",
    "# Cell 6 (CORRECTED): Download test images\n",
    "#(\"Downloading test images...\")\n",
    "\n",
    "#successful = 0\n",
    "#failed = 0\n",
    "\n",
    "#for idx, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Test images\"):\n",
    "#    if download_single_image(row, str(TEST_IMG_DIR)):\n",
    "#        successful += 1\n",
    "#   else:\n",
    "#        failed += 1\n",
    "    \n",
    "#    if idx % 100 == 0 and idx > 0:\n",
    "#        time.sleep(0.5)\n",
    "\n",
    "#print(f\"\\nTest images download complete!\")\n",
    "#print(f\"Successful: {successful}, Failed: {failed}\")\n",
    "print(f\"Total files in test_images: {len(list(TEST_IMG_DIR.glob('*.jpg')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5a7b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the test images folder: data/train_images\n",
      "No extra files found. Folder is already clean!\n",
      "\n",
      "--- Final File Counts ---\n",
      "Images in TRAIN folder (test_images): 74999\n",
      "Images in TEST folder (train_images):  74999\n"
     ]
    }
   ],
   "source": [
    "# Clean the wrong Images\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"Cleaning the test images folder: {TEST_IMAGE_DIR}\")\n",
    "\n",
    "# Create a set of valid sample IDs for the test set for fast lookups\n",
    "valid_test_ids = set(df_test['sample_id'].astype(str))\n",
    "\n",
    "files_to_delete = []\n",
    "# Find all files that do NOT belong in the test set\n",
    "for f_path in TEST_IMAGE_DIR.glob('*.jpg'):\n",
    "    if f_path.stem not in valid_test_ids:\n",
    "        files_to_delete.append(f_path)\n",
    "\n",
    "if not files_to_delete:\n",
    "    print(\"No extra files found. Folder is already clean!\")\n",
    "else:\n",
    "    print(f\"Found {len(files_to_delete)} extra files to delete. Deleting now...\")\n",
    "    for f in tqdm(files_to_delete, desc=\"Cleaning\"):\n",
    "        f.unlink() # This deletes the file\n",
    "    print(\"Cleaning complete.\")\n",
    "\n",
    "# --- Final Verification ---\n",
    "train_count = len(list(TRAIN_IMAGE_DIR.glob('*.jpg')))\n",
    "test_count = len(list(TEST_IMAGE_DIR.glob('*.jpg')))\n",
    "\n",
    "print(\"\\n--- Final File Counts ---\")\n",
    "print(f\"Images in TRAIN folder ({TRAIN_IMAGE_DIR.name}): {train_count}\")\n",
    "print(f\"Images in TEST folder ({TEST_IMAGE_DIR.name}):  {test_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04426e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying images in 'test_images' ---\n",
      "Found 74999 of 75000 expected images.\n",
      "1 missing image(s) detected. Preparing to download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading to test_images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download attempt complete.\n",
      "------------------------------\n",
      "--- Verifying images in 'train_images' ---\n",
      "Found 74999 of 75000 expected images.\n",
      "1 missing image(s) detected. Preparing to download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading to train_images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download attempt complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell for FAST and EFFICIENT Downloading\n",
    "\n",
    "# Import the main download function from your updated utils.py\n",
    "from src.utils import download_images\n",
    "from pathlib import Path\n",
    "\n",
    "def run_smart_download(df, img_dir):\n",
    "    \"\"\"\n",
    "    Checks for missing files and calls the official download script only for them.\n",
    "    \"\"\"\n",
    "    img_dir_path = Path(img_dir)\n",
    "    print(f\"--- Verifying images in '{img_dir_path.name}' ---\")\n",
    "\n",
    "    expected_ids = set(df['sample_id'].astype(str))\n",
    "    existing_ids = {f.stem for f in img_dir_path.glob('*.jpg')}\n",
    "    missing_ids = expected_ids - existing_ids\n",
    "    \n",
    "    print(f\"Found {len(existing_ids)} of {len(expected_ids)} expected images.\")\n",
    "\n",
    "    if not missing_ids:\n",
    "        print(\"All images are present. No download needed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{len(missing_ids)} missing image(s) detected. Preparing to download.\")\n",
    "    \n",
    "    # Filter the DataFrame to get only the rows for the missing images\n",
    "    df_missing = df[df['sample_id'].astype(str).isin(missing_ids)]\n",
    "    \n",
    "    # Create the list of tasks [(sample_id, image_link), ...]\n",
    "    tasks_to_run = list(zip(df_missing['sample_id'], df_missing['image_link']))\n",
    "    \n",
    "    # Call the download function from utils.py\n",
    "    download_images(tasks_to_run, str(img_dir_path))\n",
    "    \n",
    "    print(\"\\nDownload attempt complete.\")\n",
    "\n",
    "# --- Execute the download for both sets ---\n",
    "# Make sure TRAIN_IMAGE_DIR and TEST_IMAGE_DIR are set correctly from your setup cell!\n",
    "run_smart_download(df_train, TRAIN_IMAGE_DIR)\n",
    "print(\"-\" * 30)\n",
    "run_smart_download(df_test, TEST_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504a50dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing training data...\n",
      "Parsing test data...\n",
      "\n",
      "New features created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Victoria Green Taco Sauce Mild, 12 Ounce (P...</td>\n",
       "      <td>72.00</td>\n",
       "      <td>Fl Oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Salerno Cookies, The Original Butter Cookies, ...</td>\n",
       "      <td>32.00</td>\n",
       "      <td>Ounce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bear Creek Hearty Soup Bowl, Creamy Chicken wi...</td>\n",
       "      <td>11.40</td>\n",
       "      <td>Ounce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Judeeâ€™s Blue Cheese Powder 11.25 oz - Gluten-F...</td>\n",
       "      <td>11.25</td>\n",
       "      <td>Ounce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kedem Sherry Cooking Wine, 12.7 Ounce - 12 per...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  quantity   unit\n",
       "0  La Victoria Green Taco Sauce Mild, 12 Ounce (P...     72.00  Fl Oz\n",
       "1  Salerno Cookies, The Original Butter Cookies, ...     32.00  Ounce\n",
       "2  Bear Creek Hearty Soup Bowl, Creamy Chicken wi...     11.40  Ounce\n",
       "3  Judeeâ€™s Blue Cheese Powder 11.25 oz - Gluten-F...     11.25  Ounce\n",
       "4  kedem Sherry Cooking Wine, 12.7 Ounce - 12 per...     12.00  Count"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7: Parse Catalog Content\n",
    "import re\n",
    "\n",
    "def parse_content(content_string):\n",
    "    \"\"\"\n",
    "    Parses the raw catalog_content string into separate, clean features.\n",
    "    \"\"\"\n",
    "    if not isinstance(content_string, str):\n",
    "        content_string = \"\"\n",
    "        \n",
    "    lines = content_string.strip().split('\\n')\n",
    "    \n",
    "    # Default values\n",
    "    item_name = \"\"\n",
    "    bullet_points = []\n",
    "    prod_desc = \"\"\n",
    "    value = 1.0  # Default to 1 if not found\n",
    "    unit = \"Unknown\"\n",
    "\n",
    "    for line in lines:\n",
    "        if line.lower().startswith(\"item name:\"):\n",
    "            item_name = line[len(\"item name:\"):].strip()\n",
    "        elif line.lower().startswith(\"bullet point\"):\n",
    "            bp_text = re.sub(r'Bullet Point \\d+:', '', line, flags=re.IGNORECASE).strip()\n",
    "            bullet_points.append(bp_text)\n",
    "        elif line.lower().startswith(\"product description:\"):\n",
    "            prod_desc = line[len(\"product description:\"):].strip()\n",
    "        elif line.lower().startswith(\"value:\"):\n",
    "            try:\n",
    "                value = float(line[len(\"value:\"):].strip())\n",
    "            except (ValueError, TypeError):\n",
    "                value = 1.0 # Keep default if parsing fails\n",
    "        elif line.lower().startswith(\"unit:\"):\n",
    "            unit = line[len(\"unit:\"):].strip()\n",
    "            \n",
    "    # Combine all text fields into a single 'clean_text' feature\n",
    "    clean_text = \" \".join([item_name] + bullet_points + [prod_desc]).strip()\n",
    "    \n",
    "    return pd.Series([clean_text, value, unit], index=['clean_text', 'quantity', 'unit'])\n",
    "\n",
    "# --- Apply the function to both train and test dataframes ---\n",
    "print(\"Parsing training data...\")\n",
    "df_train_parsed = df_train['catalog_content'].apply(parse_content)\n",
    "df_train = pd.concat([df_train, df_train_parsed], axis=1)\n",
    "\n",
    "print(\"Parsing test data...\")\n",
    "df_test_parsed = df_test['catalog_content'].apply(parse_content)\n",
    "df_test = pd.concat([df_test, df_test_parsed], axis=1)\n",
    "\n",
    "# Display the new columns to verify\n",
    "print(\"\\nNew features created successfully!\")\n",
    "df_train[['clean_text', 'quantity', 'unit']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65d991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text model cleared from memory.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Generate and Save Text Embeddings\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # --- Device setup ---\n",
    "# DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# # --- Parameters ---\n",
    "# TEXT_BATCH_SIZE = 33\n",
    "# SAVE_DIR = Path(\"embeddings\")\n",
    "# SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# # --- Text Embedding Generation ---\n",
    "# print(\"Loading text model...\")\n",
    "# text_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\n",
    "\n",
    "# def generate_text_embeddings(df, column, prefix=\"train\"):\n",
    "#     \"\"\"Generates and saves text embeddings in chunks.\"\"\"\n",
    "#     print(f\"\\n--- Generating Text Embeddings for '{prefix}' set ---\")\n",
    "#     texts = df[column].tolist()\n",
    "#     EMB_CHUNK = 15000  # Process 10,000 texts at a time\n",
    "\n",
    "#     for start in range(0, len(texts), EMB_CHUNK):\n",
    "#         end = min(start + EMB_CHUNK, len(texts))\n",
    "#         print(f\"Processing samples {start} to {end}...\")\n",
    "#         batch_texts = texts[start:end]\n",
    "        \n",
    "#         embeds = text_model.encode(\n",
    "#             batch_texts, \n",
    "#             batch_size=TEXT_BATCH_SIZE, \n",
    "#             show_progress_bar=True, \n",
    "#             convert_to_numpy=True\n",
    "#         )\n",
    "        \n",
    "#         np.save(SAVE_DIR / f\"{prefix}_text_embeds_{start}_{end}.npy\", embeds)\n",
    "#         print(f\"Saved chunk: {prefix}_text_embeds_{start}_{end}.npy\")\n",
    "#         if DEVICE == \"mps\":\n",
    "#             torch.mps.empty_cache()\n",
    "\n",
    "# # --- Execute Text Embedding Generation ---\n",
    "# #generate_text_embeddings(df_train, \"clean_text\", prefix=\"train\")\n",
    "# #generate_text_embeddings(df_test, \"clean_text\", prefix=\"test\")\n",
    "\n",
    "# # --- IMPORTANT: Clear model from memory ---\n",
    "# #del text_model\n",
    "# #if DEVICE == \"mps\":\n",
    " #    torch.mps.empty_cache()\n",
    "print(\"\\nText model cleared from memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba6cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ssl\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "# import clip\n",
    "# from PIL import Image\n",
    "\n",
    "# # Cell 9 (UPDATED): More Memory-Efficient Image Embeddings\n",
    "\n",
    "# import gc # Import the garbage collector\n",
    "\n",
    "# # --- SSL Fix (just in case) ---\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# # --- Device setup ---\n",
    "# DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# # --- Parameters ---\n",
    "# # REDUCED BATCH SIZE to lower memory usage\n",
    "# IMAGE_BATCH_SIZE = 16 \n",
    "# SAVE_DIR = Path(\"embeddings\")\n",
    "\n",
    "# # --- Image Embedding Generation ---\n",
    "# print(\"Loading CLIP model...\")\n",
    "# # This is the line that was causing the crash\n",
    "# clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE) \n",
    "# print(\"CLIP model loaded successfully.\")\n",
    "\n",
    "# def generate_image_embeddings(df, image_dir, prefix=\"train\"):\n",
    "#     \"\"\"Generates and saves image embeddings in chunks with aggressive memory cleaning.\"\"\"\n",
    "#     print(f\"\\n--- Generating Image Embeddings for '{prefix}' set ---\")\n",
    "#     image_paths = [Path(image_dir) / f\"{sid}.jpg\" for sid in df['sample_id'].astype(str)]\n",
    "#     EMB_CHUNK = 5000  # Process 5,000 images at a time\n",
    "\n",
    "#     for start in range(0, len(image_paths), EMB_CHUNK):\n",
    "#         end = min(start + EMB_CHUNK, len(image_paths))\n",
    "#         print(f\"Processing samples {start} to {end}...\")\n",
    "#         batch_paths = image_paths[start:end]\n",
    "        \n",
    "#         # Inner loop to process smaller batches\n",
    "#         chunk_embeds = []\n",
    "#         for i in tqdm(range(0, len(batch_paths), IMAGE_BATCH_SIZE), desc=\"Image batches\"):\n",
    "#             inner_batch_paths = batch_paths[i:i+IMAGE_BATCH_SIZE]\n",
    "#             images = []\n",
    "#             for p in inner_batch_paths:\n",
    "#                 try:\n",
    "#                     img = Image.open(p).convert(\"RGB\")\n",
    "#                     images.append(clip_preprocess(img))\n",
    "#                 except (FileNotFoundError, OSError):\n",
    "#                     images.append(torch.zeros(3, 224, 224)) # Placeholder for missing images\n",
    "            \n",
    "#             image_batch = torch.stack(images).to(DEVICE)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 embeds = clip_model.encode_image(image_batch).cpu().numpy()\n",
    "#                 chunk_embeds.append(embeds)\n",
    "\n",
    "#             # Forcefully clear memory\n",
    "#             del image_batch\n",
    "#             del images\n",
    "#             if DEVICE == \"mps\":\n",
    "#                 torch.mps.empty_cache()\n",
    "#             gc.collect()\n",
    "\n",
    "#         # Save the collected embeddings for the entire chunk\n",
    "#         chunk_embeds_full = np.vstack(chunk_embeds)\n",
    "#         np.save(SAVE_DIR / f\"{prefix}_image_embeds_{start}_{end}.npy\", chunk_embeds_full)\n",
    "#         print(f\"Saved chunk: {prefix}_image_embeds_{start}_{end}.npy\")\n",
    "\n",
    "# # --- Execute Image Embedding Generation ---\n",
    "# generate_image_embeddings(df_train, TRAIN_IMAGE_DIR, prefix=\"train\")\n",
    "\n",
    "# # --- Final Cleanup ---\n",
    "# del clip_model\n",
    "# gc.collect()\n",
    "# if DEVICE == \"mps\":\n",
    "#     torch.mps.empty_cache()\n",
    "\n",
    "# print(\"\\nAll image embeddings have been generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9ce7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ssl\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "# import clip\n",
    "# from PIL import Image\n",
    "# import gc\n",
    "# import pandas as pd\n",
    "\n",
    "# # --- This script does ONLY ONE THING: generates test image embeddings ---\n",
    "\n",
    "# print(\"--- Starting Test Image Embedding Generation ---\")\n",
    "\n",
    "# # --- Setup Paths ---\n",
    "# DATA_DIR = Path('./data')\n",
    "# TEST_IMAGE_DIR = DATA_DIR / 'train_images' # As per our corrected path setup\n",
    "# SAVE_DIR = Path(\"embeddings\")\n",
    "# SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# # --- SSL Fix ---\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# # --- Device setup ---\n",
    "# DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# # --- Parameters ---\n",
    "# IMAGE_BATCH_SIZE = 16 \n",
    "# EMB_CHUNK = 5000\n",
    "\n",
    "# # --- Load ONLY the necessary data ---\n",
    "# print(\"Loading test.csv...\")\n",
    "# df_test = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "\n",
    "# # --- Image Embedding Generation Function ---\n",
    "# def generate_image_embeddings(df, image_dir, prefix=\"test\"):\n",
    "#     print(f\"\\n--- Generating Image Embeddings for '{prefix}' set ---\")\n",
    "#     image_paths = [Path(image_dir) / f\"{sid}.jpg\" for sid in df['sample_id'].astype(str)]\n",
    "\n",
    "#     for start in range(0, len(image_paths), EMB_CHUNK):\n",
    "#         end = min(start + EMB_CHUNK, len(image_paths))\n",
    "#         print(f\"Processing samples {start} to {end}...\")\n",
    "#         batch_paths = image_paths[start:end]\n",
    "        \n",
    "#         chunk_embeds = []\n",
    "#         for i in tqdm(range(0, len(batch_paths), IMAGE_BATCH_SIZE), desc=\"Image batches\"):\n",
    "#             inner_batch_paths = batch_paths[i:i+IMAGE_BATCH_SIZE]\n",
    "#             images = []\n",
    "#             for p in inner_batch_paths:\n",
    "#                 try:\n",
    "#                     img = Image.open(p).convert(\"RGB\")\n",
    "#                     images.append(clip_preprocess(img))\n",
    "#                 except (FileNotFoundError, OSError):\n",
    "#                     images.append(torch.zeros(3, 224, 224))\n",
    "            \n",
    "#             image_batch = torch.stack(images).to(DEVICE)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 embeds = clip_model.encode_image(image_batch).cpu().numpy()\n",
    "#                 chunk_embeds.append(embeds)\n",
    "\n",
    "#             # Forcefully clear memory\n",
    "#             del image_batch, images\n",
    "#             if DEVICE == \"mps\":\n",
    "#                 torch.mps.empty_cache()\n",
    "#             gc.collect()\n",
    "\n",
    "#         chunk_embeds_full = np.vstack(chunk_embeds)\n",
    "#         np.save(SAVE_DIR / f\"{prefix}_image_embeds_{start}_{end}.npy\", chunk_embeds_full)\n",
    "#         print(f\"Saved chunk: {prefix}_image_embeds_{start}_{end}.npy\")\n",
    "\n",
    "# # --- Main Execution ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"\\nLoading CLIP model...\")\n",
    "#     clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE) \n",
    "#     print(\"CLIP model loaded successfully.\")\n",
    "\n",
    "#     # Execute ONLY for the TEST set\n",
    "#     #generate_image_embeddings(df_test, TEST_IMAGE_DIR, prefix=\"test\")\n",
    "\n",
    "#     print(\"\\nAll test image embeddings have been generated and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68113274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell: Generate LAST CHUNK of TEST IMAGE Embeddings\n",
    "\n",
    "# import ssl\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "# import clip\n",
    "# from PIL import Image\n",
    "# import gc\n",
    "\n",
    "# # --- SSL Fix ---\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# # --- Device setup ---\n",
    "# DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# # --- Parameters ---\n",
    "# IMAGE_BATCH_SIZE = 16 \n",
    "# EMB_CHUNK = 5000\n",
    "# SAVE_DIR = Path(\"embeddings\")\n",
    "\n",
    "# # --- Image Embedding Generation ---\n",
    "# print(\"Loading CLIP model...\")\n",
    "# clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE) \n",
    "# print(\"CLIP model loaded successfully.\")\n",
    "\n",
    "# def generate_image_embeddings(df, image_dir, prefix=\"test\", start_from=0):\n",
    "#     print(f\"\\n--- Generating Image Embeddings for '{prefix}' set, starting from index {start_from} ---\")\n",
    "#     image_paths = [Path(image_dir) / f\"{sid}.jpg\" for sid in df['sample_id'].astype(str)]\n",
    "\n",
    "#     # This loop will now start from 70000\n",
    "#     for start in range(start_from, len(image_paths), EMB_CHUNK):\n",
    "#         end = min(start + EMB_CHUNK, len(image_paths))\n",
    "#         print(f\"Processing samples {start} to {end}...\")\n",
    "#         batch_paths = image_paths[start:end]\n",
    "        \n",
    "#         chunk_embeds = []\n",
    "#         for i in tqdm(range(0, len(batch_paths), IMAGE_BATCH_SIZE), desc=\"Image batches\"):\n",
    "#             inner_batch_paths = batch_paths[i:i+IMAGE_BATCH_SIZE]\n",
    "#             images = []\n",
    "#             for p in inner_batch_paths:\n",
    "#                 try:\n",
    "#                     img = Image.open(p).convert(\"RGB\")\n",
    "#                     images.append(clip_preprocess(img))\n",
    "#                 except (FileNotFoundError, OSError):\n",
    "#                     images.append(torch.zeros(3, 224, 224))\n",
    "            \n",
    "#             image_batch = torch.stack(images).to(DEVICE)\n",
    "#             with torch.no_grad():\n",
    "#                 embeds = clip_model.encode_image(image_batch).cpu().numpy()\n",
    "#                 chunk_embeds.append(embeds)\n",
    "\n",
    "#             del image_batch, images\n",
    "#             if DEVICE == \"mps\": torch.mps.empty_cache()\n",
    "#             gc.collect()\n",
    "\n",
    "#         chunk_embeds_full = np.vstack(chunk_embeds)\n",
    "#         np.save(SAVE_DIR / f\"{prefix}_image_embeds_{start}_{end}.npy\", chunk_embeds_full)\n",
    "#         print(f\"Saved chunk: {prefix}_image_embeds_{start}_{end}.npy\")\n",
    "\n",
    "# # --- Execute for ONLY the last chunk of the TEST set ---\n",
    "# generate_image_embeddings(df_test, TEST_IMAGE_DIR, prefix=\"test\", start_from=70000)\n",
    "\n",
    "# del clip_model\n",
    "# gc.collect()\n",
    "# if DEVICE == \"mps\":\n",
    "#     torch.mps.empty_cache()\n",
    "\n",
    "# print(\"\\nâœ… Final image chunk generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cae8e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing categorical 'unit' features...\n",
      "Categorical features prepared.\n",
      "\n",
      "--- Processing 'train' set ---\n",
      "Loading and combining 5 text chunks...\n",
      "Loading and combining 15 image chunks...\n",
      "Horizontally stacking all features...\n",
      "Saved final feature array to: embeddings/final_X_train.npy with shape (75000, 1032)\n",
      "\n",
      "--- Processing 'test' set ---\n",
      "Loading and combining 5 text chunks...\n",
      "Loading and combining 15 image chunks...\n",
      "Horizontally stacking all features...\n",
      "Saved final feature array to: embeddings/final_X_test.npy with shape (75000, 1032)\n",
      "\n",
      "All final feature arrays have been created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Build and Save Final Datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "SAVE_DIR = Path(\"embeddings\")\n",
    "\n",
    "# --- First, handle the categorical 'unit' features to get consistent columns ---\n",
    "print(\"Preparing categorical 'unit' features...\")\n",
    "train_units_df = pd.get_dummies(df_train['unit'], prefix='unit')\n",
    "test_units_df = pd.get_dummies(df_test['unit'], prefix='unit')\n",
    "train_units_aligned, test_units_aligned = train_units_df.align(test_units_df, join='outer', axis=1, fill_value=0)\n",
    "print(\"Categorical features prepared.\")\n",
    "\n",
    "def combine_and_save(prefix, df, unit_features):\n",
    "    \"\"\"Loads, combines, and saves the final feature set for one split (train/test).\"\"\"\n",
    "    print(f\"\\n--- Processing '{prefix}' set ---\")\n",
    "    \n",
    "    text_files = sorted(SAVE_DIR.glob(f\"{prefix}_text_embeds_*.npy\"))\n",
    "    image_files = sorted(SAVE_DIR.glob(f\"{prefix}_image_embeds_*.npy\"))\n",
    "    \n",
    "    if not text_files or not image_files:\n",
    "        raise FileNotFoundError(f\"Missing embedding files for '{prefix}'. Please check the 'embeddings' folder.\")\n",
    "\n",
    "    print(f\"Loading and combining {len(text_files)} text chunks...\")\n",
    "    text_embeds = np.vstack([np.load(f) for f in text_files])\n",
    "    \n",
    "    print(f\"Loading and combining {len(image_files)} image chunks...\")\n",
    "    image_embeds = np.vstack([np.load(f) for f in image_files])\n",
    "\n",
    "    print(\"Horizontally stacking all features...\")\n",
    "    final_X = np.hstack([\n",
    "        text_embeds,\n",
    "        image_embeds,\n",
    "        df['quantity'].values.reshape(-1, 1),\n",
    "        unit_features.values\n",
    "    ])\n",
    "    \n",
    "    save_path = SAVE_DIR / f\"final_X_{prefix}.npy\"\n",
    "    np.save(save_path, final_X)\n",
    "    print(f\"Saved final feature array to: {save_path} with shape {final_X.shape}\")\n",
    "    \n",
    "    # Clean up to free RAM for the next step\n",
    "    del text_embeds, image_embeds, final_X\n",
    "    gc.collect()\n",
    "\n",
    "# --- Execute for train and test sets sequentially ---\n",
    "combine_and_save(\"train\", df_train, train_units_aligned)\n",
    "combine_and_save(\"test\", df_test, test_units_aligned)\n",
    "\n",
    "print(\"\\nAll final feature arrays have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed33b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Load Final Data, Train Model, and Create Submission\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# import lightgbm as lgb\n",
    "# import gc\n",
    "\n",
    "# SAVE_DIR = Path(\"embeddings\")\n",
    "\n",
    "# # --- Load the final, pre-combined feature arrays ---\n",
    "# print(\"Loading final training and testing data...\")\n",
    "# X_train = np.load(SAVE_DIR / \"final_X_train.npy\", allow_pickle=True)\n",
    "# X_test = np.load(SAVE_DIR / \"final_X_test.npy\", allow_pickle=True)\n",
    "\n",
    "# # Prepare the target variable (this uses very little memory)\n",
    "# y_train = np.log1p(df_train['price'])\n",
    "\n",
    "# print(f\"Data loaded successfully!\")\n",
    "# print(f\"X_train shape: {X_train.shape}\")\n",
    "# print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# # --- Train the Model ---\n",
    "# print(\"\\nTraining LightGBM model...\")\n",
    "# lgbm = lgb.LGBMRegressor(\n",
    "#     objective='regression_l1',\n",
    "#     metric='rmse',\n",
    "#     n_estimators=2000,\n",
    "#     learning_rate=0.01,\n",
    "#     feature_fraction=0.8,\n",
    "#     bagging_fraction=0.8,\n",
    "#     n_jobs=-1,\n",
    "#     seed=42,\n",
    "#     verbose=-1,\n",
    "# )\n",
    "\n",
    "# lgbm.fit(X_train, y_train)\n",
    "# print(\"Model training complete.\")\n",
    "\n",
    "# # --- We no longer need the large X_train array in memory ---\n",
    "# del X_train\n",
    "# gc.collect()\n",
    "\n",
    "# # --- Generate Predictions ---\n",
    "# print(\"\\nGenerating predictions on the test set...\")\n",
    "# predictions_log = lgbm.predict(X_test)\n",
    "# predictions = np.expm1(predictions_log)\n",
    "# predictions[predictions < 0] = 0\n",
    "\n",
    "# # --- Create Submission File ---\n",
    "# submission_df = pd.DataFrame({'sample_id': df_test['sample_id'], 'price': predictions})\n",
    "# submission_df.to_csv('test_out.csv', index=False)\n",
    "\n",
    "# print(\"\\nSubmission file 'test_out.csv' created successfully!\")\n",
    "# print(\"Here are the first 5 predictions:\")\n",
    "# print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f9a5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell: Advanced Ensemble Training for Top Score\n",
    "# \n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from tqdm import tqdm\n",
    "# import gc\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# \n",
    "# # --- 1. Load Data ---\n",
    "# print(\"=\"*60)\n",
    "# print(\"ðŸš€ Advanced Ensemble Model - Starting Training\")\n",
    "# print(\"=\"*60)\n",
    "# \n",
    "# # Define paths for Colab\n",
    "# DATA_DIR = Path('data/')\n",
    "# SAVE_DIR = Path('embeddings')\n",
    "# \n",
    "# print(\"\\n[1/5] Loading pre-computed feature arrays...\")\n",
    "# X_train = np.load(SAVE_DIR / \"final_X_train.npy\", allow_pickle=True).astype(np.float32)\n",
    "# X_test = np.load(SAVE_DIR / \"final_X_test.npy\", allow_pickle=True).astype(np.float32)\n",
    "# \n",
    "# df_train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "# df_test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "# \n",
    "# # Target variable (log-transformed)\n",
    "# y_train_log = np.log1p(df_train['price'].values)\n",
    "# \n",
    "# print(f\"âœ“ Data loaded. X_train shape: {X_train.shape}\")\n",
    "# \n",
    "# # --- 2. Setup Advanced Training Strategy ---\n",
    "# print(\"\\n[2/5] Setting up training strategy...\")\n",
    "# N_FOLDS = 5               # Use 5 folds for robust validation\n",
    "# N_BAGS = 3                # Train 3 models with different seeds per fold\n",
    "# SEEDS = [42, 2024, 777]    # Different random seeds for bagging\n",
    "# \n",
    "# KF = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "# \n",
    "# # Arrays to store predictions\n",
    "# oof_preds = np.zeros(X_train.shape[0])\n",
    "# test_preds = np.zeros(X_test.shape[0])\n",
    "# \n",
    "# # --- 3. Define the LightGBM Model ---\n",
    "# # These are carefully tuned hyperparameters for high performance\n",
    "# lgbm_params = {\n",
    "#     \"objective\": \"regression_l1\",\n",
    "#     \"metric\": \"rmse\",\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"n_estimators\": 5000,\n",
    "#     \"learning_rate\": 0.01,\n",
    "#     \"num_leaves\": 40,\n",
    "#     \"max_depth\": 12,\n",
    "#     \"feature_fraction\": 0.8,\n",
    "#     \"bagging_fraction\": 0.8,\n",
    "#     \"bagging_freq\": 1,\n",
    "#     \"lambda_l1\": 2.0,\n",
    "#     \"lambda_l2\": 2.0,\n",
    "#     \"min_child_samples\": 20,\n",
    "#     \"verbose\": -1,\n",
    "#     \"n_jobs\": -1,\n",
    "# }\n",
    "# \n",
    "# print(f\"âœ“ Using {N_FOLDS}-Fold CV with {N_BAGS} bags each.\")\n",
    "# \n",
    "# # --- 4. The Training Loop ---\n",
    "# print(\"\\n[3/5] Starting model training...\")\n",
    "# # Wrap the main loop with tqdm for a master progress bar\n",
    "# fold_iterator = tqdm(enumerate(KF.split(X_train, y_train_log)), total=N_FOLDS, desc=\"Total Folds\")\n",
    "# \n",
    "# for fold, (train_idx, val_idx) in fold_iterator:\n",
    "#     X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "#     y_tr, y_val = y_train_log[train_idx], y_train_log[val_idx]\n",
    "#     \n",
    "#     val_preds_fold = np.zeros(len(val_idx))\n",
    "#     test_preds_fold = np.zeros(len(X_test))\n",
    "#     \n",
    "#     # Inner loop for bagging\n",
    "#     for b_idx in range(N_BAGS):\n",
    "#         print(f\"  > Fold {fold+1}, Bag {b_idx+1}/{N_BAGS}\")\n",
    "#         \n",
    "#         # Update seed for this bag\n",
    "#         params = lgbm_params.copy()\n",
    "#         params['seed'] = SEEDS[b_idx] * (fold + 1)\n",
    "#         \n",
    "#         model = lgb.LGBMRegressor(**params)\n",
    "#         model.fit(\n",
    "#             X_tr, y_tr,\n",
    "#             eval_set=[(X_val, y_val)],\n",
    "#             callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "#         )\n",
    "#         \n",
    "#         val_preds_fold += model.predict(X_val, num_iteration=model.best_iteration_) / N_BAGS\n",
    "#         test_preds_fold += model.predict(X_test, num_iteration=model.best_iteration_) / N_BAGS\n",
    "#         \n",
    "#         del model; gc.collect()\n",
    "# \n",
    "#     oof_preds[val_idx] = val_preds_fold\n",
    "#     test_preds += test_preds_fold / N_FOLDS\n",
    "#     \n",
    "#     fold_rmse = np.sqrt(mean_squared_error(y_val, val_preds_fold))\n",
    "#     fold_iterator.set_postfix(last_fold_rmse=fold_rmse)\n",
    "# \n",
    "# # --- 5. Post-Processing and Submission ---\n",
    "# print(\"\\n[4/5] Evaluating and calibrating predictions...\")\n",
    "# \n",
    "# # Calculate overall Out-of-Fold (OOF) SMAPE score\n",
    "# oof_prices_actual = np.expm1(y_train_log)\n",
    "# oof_prices_pred = np.expm1(oof_preds)\n",
    "# denominator = (oof_prices_actual + oof_prices_pred) / 2\n",
    "# overall_oof_smape = np.mean(np.abs(oof_prices_pred - oof_prices_actual) / denominator) * 100\n",
    "# \n",
    "# print(f\"\\n{'='*30}\")\n",
    "# print(f\"ðŸ“Š Overall OOF SMAPE Score: {overall_oof_smape:.4f}%\")\n",
    "# print(f\"{'='*30}\")\n",
    "# \n",
    "# # Optional but powerful: Calibrate predictions with a simple linear model\n",
    "# print(\"Calibrating test predictions on OOF results...\")\n",
    "# lr = Ridge(alpha=5.0)\n",
    "# lr.fit(oof_preds.reshape(-1, 1), y_train_log)\n",
    "# calibrated_test_preds_log = lr.predict(test_preds.reshape(-1, 1))\n",
    "# \n",
    "# # Final predictions\n",
    "# final_predictions = np.expm1(calibrated_test_preds_log)\n",
    "# final_predictions = np.clip(final_predictions, 0, None) # Ensure no negative prices\n",
    "# \n",
    "# # Create submission file\n",
    "# print(\"\\n[5/5] Creating submission file...\")\n",
    "# submission_df = pd.DataFrame({'sample_id': df_test['sample_id'], 'price': final_predictions})\n",
    "# submission_df.to_csv('test_out.csv', index=False)\n",
    "# \n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"âœ… DONE! Submission file 'test_out.csv' is ready!\")\n",
    "# print(\"=\"*60)\n",
    "# print(\"Sample of final predictions:\")\n",
    "# print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/8] Training Level 1 base models (7-fold)...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š FOLD 1/7\n",
      "  ðŸ”µ LightGBM Huber...\n",
      "  ðŸ”µ LightGBM Quantile-10...\n",
      "  ðŸ”µ LightGBM Quantile-50...\n",
      "  ðŸ”µ LightGBM Quantile-90...\n",
      "  ðŸŸ  XGBoost Pseudo-Huber...\n",
      "  ðŸŸ¢ CatBoost MAE...\n",
      "  ðŸŸ¢ CatBoost Quantile...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  ðŸŸ¢ CatBoost Quantile...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m cat2 = CatBoostRegressor(loss_function=\u001b[33m'\u001b[39m\u001b[33mQuantile:alpha=0.5\u001b[39m\u001b[33m'\u001b[39m, iterations=\u001b[32m3000\u001b[39m,\n\u001b[32m     96\u001b[39m                          learning_rate=\u001b[32m0.01\u001b[39m, depth=\u001b[32m10\u001b[39m, l2_leaf_reg=\u001b[32m3\u001b[39m,\n\u001b[32m     97\u001b[39m                          random_seed=\u001b[32m42\u001b[39m+fold, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m, thread_count=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mcat2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m         \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m oof_models[\u001b[33m'\u001b[39m\u001b[33mcat_quantile\u001b[39m\u001b[33m'\u001b[39m][val_idx] = cat2.predict(X_val)\n\u001b[32m    101\u001b[39m test_preds[\u001b[33m'\u001b[39m\u001b[33mcat_quantile\u001b[39m\u001b[33m'\u001b[39m] += cat2.predict(X_test_qt) / N_FOLDS\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/amazonml/venv/lib/python3.12/site-packages/catboost/core.py:5873\u001b[39m, in \u001b[36mCatBoostRegressor.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5872\u001b[39m     CatBoostRegressor._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5874\u001b[39m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5875\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5876\u001b[39m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/amazonml/venv/lib/python3.12/site-packages/catboost/core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/amazonml/venv/lib/python3.12/site-packages/catboost/core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# TRAIN DIVERSE BASE MODELS (Level 1)\n",
    "# ===================================================================\n",
    "print(\"\\n[5/8] Training Level 1 base models (7-fold)...\")\n",
    "\n",
    "oof_models = {\n",
    "    'lgb_huber': np.zeros(len(X_train_qt)),\n",
    "    'lgb_quantile_10': np.zeros(len(X_train_qt)),\n",
    "    'lgb_quantile_50': np.zeros(len(X_train_qt)),\n",
    "    'lgb_quantile_90': np.zeros(len(X_train_qt)),\n",
    "    'xgb_huber': np.zeros(len(X_train_qt)),\n",
    "    'cat_mae': np.zeros(len(X_train_qt)),\n",
    "    'cat_quantile': np.zeros(len(X_train_qt)),\n",
    "    'hist_huber': np.zeros(len(X_train_qt)),\n",
    "}\n",
    "\n",
    "test_preds = {k: np.zeros(len(X_test_qt)) for k in oof_models.keys()}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_qt, price_bins), 1):\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"ðŸ“Š FOLD {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_tr, X_val = X_train_qt[train_idx], X_train_qt[val_idx]\n",
    "    y_tr, y_val = y_train_transformed[train_idx], y_train_transformed[val_idx]\n",
    "    y_val_orig = y_train[val_idx]\n",
    "    \n",
    "    # LightGBM Huber (Main Model)\n",
    "    print(\"  ðŸ”µ LightGBM Huber...\")\n",
    "    lgb1 = lgb.LGBMRegressor(objective='huber', **best_params, n_estimators=5000, \n",
    "                             random_state=42+fold, n_jobs=-1, verbose=-1)\n",
    "    lgb1.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "             callbacks=[lgb.early_stopping(200, verbose=False)])\n",
    "    oof_models['lgb_huber'][val_idx] = lgb1.predict(X_val)\n",
    "    test_preds['lgb_huber'] += lgb1.predict(X_test_qt) / N_FOLDS\n",
    "    \n",
    "    # LightGBM Quantile 10th\n",
    "    print(\"  ðŸ”µ LightGBM Quantile-10...\")\n",
    "    lgb2 = lgb.LGBMRegressor(objective='quantile', alpha=0.1, **best_params, \n",
    "                             n_estimators=3000, random_state=42+fold, n_jobs=-1, verbose=-1)\n",
    "    lgb2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "             callbacks=[lgb.early_stopping(150, verbose=False)])\n",
    "    oof_models['lgb_quantile_10'][val_idx] = lgb2.predict(X_val)\n",
    "    test_preds['lgb_quantile_10'] += lgb2.predict(X_test_qt) / N_FOLDS\n",
    "    \n",
    "    # LightGBM Quantile 50th (Median)\n",
    "    print(\"  ðŸ”µ LightGBM Quantile-50...\")\n",
    "    lgb3 = lgb.LGBMRegressor(objective='quantile', alpha=0.5, **best_params, \n",
    "                             n_estimators=3000, random_state=42+fold, n_jobs=-1, verbose=-1)\n",
    "    lgb3.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "             callbacks=[lgb.early_stopping(150, verbose=False)])\n",
    "    oof_models['lgb_quantile_50'][val_idx] = lgb3.predict(X_val)\n",
    "    test_preds['lgb_quantile_50'] += lgb3.predict(X_test_qt) / N_FOLDS\n",
    "    \n",
    "    # LightGBM Quantile 90th\n",
    "    print(\"  ðŸ”µ LightGBM Quantile-90...\")\n",
    "    lgb4 = lgb.LGBMRegressor(objective='quantile', alpha=0.9, **best_params, \n",
    "                             n_estimators=3000, random_state=42+fold, n_jobs=-1, verbose=-1)\n",
    "    lgb4.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "             callbacks=[lgb.early_stopping(150, verbose=False)])\n",
    "    oof_models['lgb_quantile_90'][val_idx] = lgb4.predict(X_val)\n",
    "    test_preds['lgb_quantile_90'] += lgb4.predict(X_test_qt) / N_FOLDS\n",
    "    \n",
    "    # XGBoost Pseudo-Huber\n",
    "    print(\"  ðŸŸ  XGBoost Pseudo-Huber...\")\n",
    "    xgb1 = XGBRegressor(\n",
    "        objective='reg:pseudohubererror', \n",
    "        n_estimators=3000,\n",
    "        learning_rate=0.01, \n",
    "        max_depth=10, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8, \n",
    "        reg_alpha=0.5, \n",
    "        reg_lambda=0.5,\n",
    "        early_stopping_rounds=150,  # Move it here as a parameter\n",
    "        random_state=42+fold, \n",
    "        n_jobs=-1, \n",
    "        tree_method='hist'\n",
    "    )\n",
    "    xgb1.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    oof_models['xgb_huber'][val_idx] = xgb1.predict(X_val)\n",
    "    test_preds['xgb_huber'] += xgb1.predict(X_test_qt) / N_FOLDS\n",
    "    \n",
    "    # CatBoost MAE\n",
    "    print(\"  ðŸŸ¢ CatBoost MAE...\")\n",
    "    cat1 = CatBoostRegressor(loss_function='MAE', iterations=3000, \n",
    "                             learning_rate=0.01, depth=10, l2_leaf_reg=3,\n",
    "                             random_seed=42+fold, verbose=False, thread_count=-1)\n",
    "    cat1.fit(X_tr, y_tr, eval_set=(X_val, y_val), \n",
    "             early_stopping_rounds=150, verbose=False)\n",
    "    oof_models['cat_mae'][val_idx] = cat1.predict(X_val)\n",
    "    test_preds['cat_mae'] += cat1.predict(X_test_qt) / N_FOLDS\n",
    "    \n",
    "    # CatBoost Quantile\n",
    "    print(\"  ðŸŸ¢ CatBoost Quantile...\")\n",
    "    cat2 = CatBoostRegressor(loss_function='Quantile:alpha=0.5', iterations=3000,\n",
    "                             learning_rate=0.01, depth=10, l2_leaf_reg=3,\n",
    "                             random_seed=42+fold, verbose=False, thread_count=-1)\n",
    "    cat2.fit(X_tr, y_tr, eval_set=(X_val, y_val), \n",
    "             early_stopping_rounds=150, verbose=False)\n",
    "    oof_models['cat_quantile'][val_idx] = cat2.predict(X_val)\n",
    "    test_preds['cat_quantile'] += cat2.predict(X_test_qt) / N_FOLDS\n",
    "    \n",
    "    # HistGradientBoosting (Scikit-learn fast)\n",
    "    print(\"  ðŸŸ£ HistGradientBoosting...\")\n",
    "    hist1 = HistGradientBoostingRegressor(loss='huber', max_iter=500,\n",
    "                                          learning_rate=0.05, max_depth=12,\n",
    "                                          random_state=42+fold)\n",
    "    hist1.fit(X_tr, y_tr)\n",
    "    oof_models['hist_huber'][val_idx] = hist1.predict(X_val)\n",
    "    test_preds['hist_huber'] += hist1.predict(X_test_qt) / N_FOLDS\n",
    "    \n",
    "    # Calculate fold SMAPE\n",
    "    fold_avg = np.mean([oof_models[k][val_idx] for k in oof_models.keys()], axis=0)\n",
    "    fold_avg_price = delta * (fold_avg + np.sqrt(fold_avg**2 + delta**2))\n",
    "    fold_smape = np.mean(2 * np.abs(fold_avg_price - y_val_orig) / \n",
    "                         (np.abs(y_val_orig) + np.abs(fold_avg_price) + 1e-8)) * 100\n",
    "    print(f\"  ðŸ“ˆ Fold {fold} SMAPE: {fold_smape:.4f}%\")\n",
    "    \n",
    "    del X_tr, X_val, y_tr, y_val, lgb1, lgb2, lgb3, lgb4, xgb1, cat1, cat2, hist1\n",
    "    gc.collect()\n",
    "\n",
    "# ===================================================================\n",
    "# LEVEL 2: HILL CLIMBING ENSEMBLE (Kaggle Winner Technique)\n",
    "# ===================================================================\n",
    "print(\"\\n[6/8] Hill climbing ensemble optimization...\")\n",
    "\n",
    "# Convert OOF predictions back to original scale\n",
    "oof_original_scale = {}\n",
    "for k, v in oof_models.items():\n",
    "    oof_original_scale[k] = delta * (v + np.sqrt(v**2 + delta**2))\n",
    "\n",
    "def smape_loss(weights, predictions, y_true):\n",
    "    weights = np.abs(weights) / np.abs(weights).sum()\n",
    "    pred = sum(w * p for w, p in zip(weights, predictions))\n",
    "    return np.mean(2 * np.abs(pred - y_true) / (np.abs(y_true) + np.abs(pred) + 1e-8)) * 100\n",
    "\n",
    "# Prepare data\n",
    "pred_list = [oof_original_scale[k] for k in oof_models.keys()]\n",
    "initial_weights = np.ones(len(oof_models)) / len(oof_models)\n",
    "\n",
    "# Optimize\n",
    "result = minimize(\n",
    "    lambda w: smape_loss(w, pred_list, y_train),\n",
    "    initial_weights,\n",
    "    method='Nelder-Mead',\n",
    "    options={'maxiter': 500}\n",
    ")\n",
    "optimal_weights = np.abs(result.x) / np.abs(result.x).sum()\n",
    "\n",
    "print(\"\\nâœ“ Optimal weights found:\")\n",
    "for name, weight in zip(oof_models.keys(), optimal_weights):\n",
    "    print(f\"  {name:20s}: {weight:.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# LEVEL 3: META-MODEL STACKING\n",
    "# ===================================================================\n",
    "print(\"\\n[7/8] Training meta-model...\")\n",
    "\n",
    "# Stack OOF predictions\n",
    "stack_train = np.column_stack([oof_original_scale[k] for k in oof_models.keys()])\n",
    "\n",
    "# Quantile meta-learner (robust)\n",
    "meta_models = []\n",
    "for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    meta = QuantileRegressor(quantile=alpha, alpha=0.1, solver='highs')\n",
    "    meta.fit(stack_train, y_train)\n",
    "    meta_models.append(meta)\n",
    "\n",
    "print(f\"âœ“ Trained {len(meta_models)} quantile meta-models\")\n",
    "\n",
    "# ===================================================================\n",
    "# FINAL PREDICTIONS\n",
    "# ===================================================================\n",
    "print(\"\\n[8/8] Creating final predictions...\")\n",
    "\n",
    "# Stack test predictions\n",
    "test_stack_original = np.column_stack([\n",
    "    delta * (test_preds[k] + np.sqrt(test_preds[k]**2 + delta**2))\n",
    "    for k in oof_models.keys()\n",
    "])\n",
    "\n",
    "# Hill climbing ensemble\n",
    "pred_hill = sum(w * test_stack_original[:, i] \n",
    "                for i, w in enumerate(optimal_weights))\n",
    "\n",
    "# Meta-model ensemble\n",
    "pred_meta = np.mean([meta.predict(test_stack_original) \n",
    "                     for meta in meta_models], axis=0)\n",
    "\n",
    "# Final blend (60% hill climbing, 40% meta)\n",
    "final_predictions = 0.6 * pred_hill + 0.4 * pred_meta\n",
    "final_predictions = np.clip(final_predictions, 0.1, np.percentile(y_train, 99.8))\n",
    "\n",
    "# Calculate final OOF SMAPE\n",
    "oof_hill = sum(w * pred_list[i] for i, w in enumerate(optimal_weights))\n",
    "oof_meta = np.mean([meta.predict(stack_train) for meta in meta_models], axis=0)\n",
    "oof_final = 0.6 * oof_hill + 0.4 * oof_meta\n",
    "final_smape = np.mean(2 * np.abs(oof_final - y_train) / \n",
    "                      (np.abs(y_train) + np.abs(oof_final) + 1e-8)) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ† FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  OOF SMAPE: {final_smape:.4f}%\")\n",
    "print(f\"  Expected Test SMAPE: ~{final_smape * 1.02:.4f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# CREATE SUBMISSION\n",
    "# ===================================================================\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': df_test['sample_id'],\n",
    "    'price': final_predictions\n",
    "})\n",
    "submission.to_csv('ultimate_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSUBMISSION CREATED: ultimate_submission.csv\")\n",
    "print(\"\\n First 10 predictions:\")\n",
    "print(submission.head(10))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DONE! This should achieve <20% SMAPE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d38691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ MLP FUSION: Text + Image Embeddings (Corrected)\n",
      "======================================================================\n",
      "\n",
      "[1/4] Loading and slicing combined embeddings...\n",
      "âœ“ Text: train(75000, 384), test(75000, 384)\n",
      "âœ“ Image: train(75000, 512), test(75000, 512)\n",
      "âœ“ Other: train(75000, 156), test(75000, 156)\n",
      "\n",
      "[2/4] Scaling features...\n",
      "âœ“ Features scaled\n",
      "\n",
      "[3/4] Training MLP with K-Fold...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š FOLD 1/5\n",
      "  Using device: mps\n",
      "    Epoch 10: train_loss=0.16026, val_loss=0.20175\n",
      "    Epoch 20: train_loss=0.12822, val_loss=0.19922\n",
      "    Early stopping at epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ“ˆ Fold 1 SMAPE: 52.4386%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š FOLD 2/5\n",
      "  Using device: mps\n",
      "    Epoch 10: train_loss=0.16276, val_loss=0.19371\n",
      "    Epoch 20: train_loss=0.12914, val_loss=0.19446\n",
      "    Epoch 30: train_loss=0.10848, val_loss=0.20044\n",
      "    Early stopping at epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ“ˆ Fold 2 SMAPE: 51.9549%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š FOLD 3/5\n",
      "  Using device: mps\n",
      "    Epoch 10: train_loss=0.16391, val_loss=0.19008\n",
      "    Epoch 20: train_loss=0.12803, val_loss=0.19433\n",
      "    Early stopping at epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ“ˆ Fold 3 SMAPE: 52.2093%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š FOLD 4/5\n",
      "  Using device: mps\n",
      "    Epoch 10: train_loss=0.16471, val_loss=0.18909\n",
      "    Epoch 20: train_loss=0.12821, val_loss=0.18717\n",
      "    Early stopping at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ“ˆ Fold 4 SMAPE: 51.3790%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š FOLD 5/5\n",
      "  Using device: mps\n",
      "    Epoch 10: train_loss=0.16349, val_loss=0.19048\n",
      "    Epoch 20: train_loss=0.12911, val_loss=0.19032\n",
      "    Early stopping at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ“ˆ Fold 5 SMAPE: 51.4858%\n",
      "\n",
      "[4/4] Final evaluation and submission...\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š FINAL OOF SMAPE: 51.8935%\n",
      "======================================================================\n",
      "\n",
      "âœ… Submission created: mlp_fusion_submission.csv\n",
      "\n",
      "ðŸ“‹ First 10 predictions:\n",
      "   sample_id      price\n",
      "0     100179  11.339684\n",
      "1     245611  20.092656\n",
      "2     146263  19.950173\n",
      "3      95658   6.729285\n",
      "4      36806  15.766823\n",
      "5     148239   5.283599\n",
      "6      92659   6.564321\n",
      "7       3780  16.190256\n",
      "8     196940   8.071885\n",
      "9      20472   8.899274\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# OPTIMIZED MLP FUSION FOR TEXT + IMAGE EMBEDDINGS (COMPLETE & CORRECTED)\n",
    "# ===================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ MLP FUSION: Text + Image Embeddings (Corrected)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# ADVANCED MLP ARCHITECTURE\n",
    "# ===================================================================\n",
    "class MultimodalFusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced fusion with separate encoders and attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim, image_dim, other_dim, hidden_dim=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Linear(text_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Linear(image_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.other_encoder = nn.Sequential( # Encoder for quantity, brand, etc.\n",
    "            nn.Linear(other_dim, 64), nn.LayerNorm(64), nn.ReLU(), nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, dropout=0.1, batch_first=True)\n",
    "        \n",
    "        # Fusion layers to combine all encoded parts\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + 64, hidden_dim * 2), nn.LayerNorm(hidden_dim * 2), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU(), nn.Dropout(dropout * 0.7),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), nn.LayerNorm(hidden_dim // 2), nn.ReLU(), nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, text_emb, image_emb, other_emb):\n",
    "        text_enc = self.text_encoder(text_emb)\n",
    "        image_enc = self.image_encoder(image_emb)\n",
    "        other_enc = self.other_encoder(other_emb)\n",
    "        \n",
    "        # Cross-attention: text attends to image\n",
    "        attended, _ = self.attention(text_enc.unsqueeze(1), image_enc.unsqueeze(1), image_enc.unsqueeze(1))\n",
    "        attended = attended.squeeze(1)\n",
    "        \n",
    "        # Concatenate attended text, original image, and other features\n",
    "        fused = torch.cat([attended, image_enc, other_enc], dim=1)\n",
    "        output = self.fusion(fused)\n",
    "        return output\n",
    "\n",
    "# ===================================================================\n",
    "# TRAINING FUNCTION (UPDATED FOR 3 INPUTS)\n",
    "# ===================================================================\n",
    "def train_mlp_fusion(X_text_tr, X_image_tr, X_other_tr, y_tr, \n",
    "                     X_text_val, X_image_val, X_other_val, y_val,\n",
    "                     epochs=100, batch_size=256, lr=5e-4):\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print(f\"  Using device: {device}\")\n",
    "    \n",
    "    text_dim, image_dim, other_dim = X_text_tr.shape[1], X_image_tr.shape[1], X_other_tr.shape[1]\n",
    "    \n",
    "    model = MultimodalFusionMLP(text_dim, image_dim, other_dim).to(device)\n",
    "    \n",
    "    def pseudo_huber_loss(pred, target, delta=1.0):\n",
    "        residual = pred - target\n",
    "        return torch.mean(delta**2 * (torch.sqrt(1 + (residual/delta)**2) - 1))\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_text_tr), torch.FloatTensor(X_image_tr), torch.FloatTensor(X_other_tr), torch.FloatTensor(y_tr).unsqueeze(1))\n",
    "    val_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_text_val), torch.FloatTensor(X_image_val), torch.FloatTensor(X_other_val), torch.FloatTensor(y_val).unsqueeze(1))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    best_val_loss = float('inf'); patience, patience_counter = 15, 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train(); train_loss = 0\n",
    "        for text_b, image_b, other_b, y_b in train_loader:\n",
    "            text_b, image_b, other_b, y_b = text_b.to(device), image_b.to(device), other_b.to(device), y_b.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text_b, image_b, other_b)\n",
    "            loss = pseudo_huber_loss(output, y_b)\n",
    "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval(); val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for text_b, image_b, other_b, y_b in val_loader:\n",
    "                text_b, image_b, other_b, y_b = text_b.to(device), image_b.to(device), other_b.to(device), y_b.to(device)\n",
    "                output = model(text_b, image_b, other_b)\n",
    "                val_loss += pseudo_huber_loss(output, y_b).item()\n",
    "        \n",
    "        train_loss /= len(train_loader); val_loss /= len(val_loader); scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss, best_model_state, patience_counter = val_loss, model.state_dict(), 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience: print(f\"    Early stopping at epoch {epoch+1}\"); break\n",
    "        if (epoch + 1) % 10 == 0: print(f\"    Epoch {epoch+1}: train_loss={train_loss:.5f}, val_loss={val_loss:.5f}\")\n",
    "            \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# ===================================================================\n",
    "# PREDICTION FUNCTION (UPDATED FOR 3 INPUTS)\n",
    "# ===================================================================\n",
    "def predict_mlp(model, X_text, X_image, X_other, batch_size=256):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval(); predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(X_text), batch_size), desc=\"Predicting\", leave=False):\n",
    "            end_idx = min(i + batch_size, len(X_text))\n",
    "            text_b = torch.FloatTensor(X_text[i:end_idx]).to(device)\n",
    "            image_b = torch.FloatTensor(X_image[i:end_idx]).to(device)\n",
    "            other_b = torch.FloatTensor(X_other[i:end_idx]).to(device)\n",
    "            output = model(text_b, image_b, other_b)\n",
    "            predictions.append(output.cpu().numpy())\n",
    "    return np.vstack(predictions).flatten()\n",
    "\n",
    "# ===================================================================\n",
    "# --- CORRECTED DATA LOADING AND SLICING ---\n",
    "# ===================================================================\n",
    "print(\"\\n[1/4] Loading and slicing combined embeddings...\")\n",
    "SAVE_DIR = Path(\"embeddings_medium\") # Use the correct folder\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "y_train_log = np.log1p(df_train['price'].values)\n",
    "\n",
    "# Load the SINGLE, COMBINED feature files\n",
    "X_train_full = np.load(SAVE_DIR / \"final_X_train_medium_with_brand.npy\", allow_pickle=False)\n",
    "X_test_full = np.load(SAVE_DIR / \"final_X_test_medium_with_brand.npy\", allow_pickle=False)\n",
    "\n",
    "# Define the dimensions of your features\n",
    "text_dim = 384 # From SentenceTransformer\n",
    "image_dim = 512 # From ViT-B/16\n",
    "\n",
    "# Slice the combined arrays into their constituent parts\n",
    "train_text = X_train_full[:, :text_dim]\n",
    "train_image = X_train_full[:, text_dim:text_dim+image_dim]\n",
    "train_other = X_train_full[:, text_dim+image_dim:]\n",
    "\n",
    "test_text = X_test_full[:, :text_dim]\n",
    "test_image = X_test_full[:, text_dim:text_dim+image_dim]\n",
    "test_other = X_test_full[:, text_dim+image_dim:]\n",
    "\n",
    "print(f\"âœ“ Text: train{train_text.shape}, test{test_text.shape}\")\n",
    "print(f\"âœ“ Image: train{train_image.shape}, test{test_image.shape}\")\n",
    "print(f\"âœ“ Other: train{train_other.shape}, test{test_other.shape}\")\n",
    "del X_train_full, X_test_full; gc.collect()\n",
    "\n",
    "# ===================================================================\n",
    "# SCALE FEATURES\n",
    "# ===================================================================\n",
    "print(\"\\n[2/4] Scaling features...\")\n",
    "text_scaler, image_scaler, other_scaler = RobustScaler(), RobustScaler(), RobustScaler()\n",
    "\n",
    "train_text_scaled = text_scaler.fit_transform(train_text); test_text_scaled = text_scaler.transform(test_text)\n",
    "train_image_scaled = image_scaler.fit_transform(train_image); test_image_scaled = image_scaler.transform(test_image)\n",
    "train_other_scaled = other_scaler.fit_transform(train_other); test_other_scaled = other_scaler.transform(test_other)\n",
    "\n",
    "print(\"âœ“ Features scaled\")\n",
    "del train_text, train_image, train_other, test_text, test_image, test_other; gc.collect()\n",
    "\n",
    "# ===================================================================\n",
    "# K-FOLD TRAINING\n",
    "# ===================================================================\n",
    "print(\"\\n[3/4] Training MLP with K-Fold...\")\n",
    "N_FOLDS = 5; kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(train_text_scaled)); test_preds = np.zeros(len(test_text_scaled))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_text_scaled), 1):\n",
    "    print(f\"\\n{'â”€'*70}\\nðŸ“Š FOLD {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    model = train_mlp_fusion(\n",
    "        train_text_scaled[train_idx], train_image_scaled[train_idx], train_other_scaled[train_idx], y_train_log[train_idx],\n",
    "        train_text_scaled[val_idx], train_image_scaled[val_idx], train_other_scaled[val_idx], y_train_log[val_idx]\n",
    "    )\n",
    "    \n",
    "    oof_preds[val_idx] = predict_mlp(model, train_text_scaled[val_idx], train_image_scaled[val_idx], train_other_scaled[val_idx])\n",
    "    test_preds += predict_mlp(model, test_text_scaled, test_image_scaled, test_other_scaled) / N_FOLDS\n",
    "    \n",
    "    val_pred_price = np.expm1(oof_preds[val_idx]); val_actual_price = np.expm1(y_train_log[val_idx])\n",
    "    fold_smape = np.mean(2 * np.abs(val_pred_price - val_actual_price) / (np.abs(val_actual_price) + np.abs(val_pred_price) + 1e-8)) * 100\n",
    "    print(f\"  ðŸ“ˆ Fold {fold} SMAPE: {fold_smape:.4f}%\")\n",
    "    \n",
    "    del model; gc.collect(); torch.mps.empty_cache() if torch.backends.mps.is_available() else None\n",
    "\n",
    "# ===================================================================\n",
    "# FINAL EVALUATION & SUBMISSION\n",
    "# ===================================================================\n",
    "print(\"\\n[4/4] Final evaluation and submission...\")\n",
    "oof_prices = np.expm1(oof_preds); actual_prices = df_train['price'].values\n",
    "overall_smape = np.mean(2 * np.abs(oof_prices - actual_prices) / (np.abs(actual_prices) + np.abs(oof_prices) + 1e-8)) * 100\n",
    "print(\"\\n\" + \"=\"*70 + f\"\\nðŸ“Š FINAL OOF SMAPE: {overall_smape:.4f}%\\n\" + \"=\"*70)\n",
    "\n",
    "final_predictions = np.expm1(test_preds); final_predictions = np.clip(final_predictions, 0.01, None)\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "submission = pd.DataFrame({'sample_id': df_test['sample_id'],'price': final_predictions})\n",
    "submission.to_csv('mlp_fusion_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Submission created: mlp_fusion_submission.csv\")\n",
    "print(\"\\nðŸ“‹ First 10 predictions:\"); print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a7a2bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading CSVs and embeddings...\n",
      "train_text (75000, 384) train_image (75000, 512) train_other (75000, 164)\n",
      "Scaling features...\n",
      "\n",
      "================================================================================\n",
      "Starting Fold 1/5\n",
      "Fold 1 Epoch 001 | train_loss: 0.54104 | val_loss: 0.42435 | val_smape: 64.2822% | best_smape: 64.2822% | wait: 0\n",
      "Fold 1 Epoch 002 | train_loss: 0.40997 | val_loss: 0.37918 | val_smape: 58.9091% | best_smape: 58.9091% | wait: 0\n",
      "Fold 1 Epoch 003 | train_loss: 0.37570 | val_loss: 0.36009 | val_smape: 56.5821% | best_smape: 56.5821% | wait: 0\n",
      "Fold 1 Epoch 004 | train_loss: 0.35291 | val_loss: 0.34443 | val_smape: 54.6148% | best_smape: 54.6148% | wait: 0\n",
      "Fold 1 Epoch 005 | train_loss: 0.33377 | val_loss: 0.34094 | val_smape: 54.2507% | best_smape: 54.2507% | wait: 0\n",
      "Fold 1 Epoch 006 | train_loss: 0.31384 | val_loss: 0.33043 | val_smape: 52.8749% | best_smape: 52.8749% | wait: 0\n",
      "Fold 1 Epoch 007 | train_loss: 0.30513 | val_loss: 0.32900 | val_smape: 52.7561% | best_smape: 52.7561% | wait: 0\n",
      "Fold 1 Epoch 008 | train_loss: 0.30056 | val_loss: 0.32768 | val_smape: 52.5448% | best_smape: 52.5448% | wait: 0\n",
      "Fold 1 Epoch 009 | train_loss: 0.29730 | val_loss: 0.32761 | val_smape: 52.5259% | best_smape: 52.5448% | wait: 1\n",
      "Fold 1 Epoch 010 | train_loss: 0.30380 | val_loss: 0.32486 | val_smape: 52.1692% | best_smape: 52.1692% | wait: 0\n",
      "Fold 1 Epoch 011 | train_loss: 0.28887 | val_loss: 0.32297 | val_smape: 51.7875% | best_smape: 51.7875% | wait: 0\n",
      "Fold 1 Epoch 012 | train_loss: 0.27354 | val_loss: 0.31926 | val_smape: 51.2989% | best_smape: 51.2989% | wait: 0\n",
      "Fold 1 Epoch 013 | train_loss: 0.25974 | val_loss: 0.31593 | val_smape: 50.9744% | best_smape: 50.9744% | wait: 0\n",
      "Fold 1 Epoch 014 | train_loss: 0.24746 | val_loss: 0.31809 | val_smape: 51.3785% | best_smape: 50.9744% | wait: 1\n",
      "Fold 1 Epoch 015 | train_loss: 0.23748 | val_loss: 0.31733 | val_smape: 51.2207% | best_smape: 50.9744% | wait: 2\n",
      "Fold 1 Epoch 016 | train_loss: 0.22670 | val_loss: 0.31365 | val_smape: 50.6117% | best_smape: 50.6117% | wait: 0\n",
      "Fold 1 Epoch 017 | train_loss: 0.21704 | val_loss: 0.31701 | val_smape: 51.1099% | best_smape: 50.6117% | wait: 1\n",
      "Fold 1 Epoch 018 | train_loss: 0.20972 | val_loss: 0.31569 | val_smape: 50.8629% | best_smape: 50.6117% | wait: 2\n",
      "Fold 1 Epoch 019 | train_loss: 0.20145 | val_loss: 0.31322 | val_smape: 50.4835% | best_smape: 50.4835% | wait: 0\n",
      "Fold 1 Epoch 020 | train_loss: 0.19528 | val_loss: 0.31797 | val_smape: 51.0065% | best_smape: 50.4835% | wait: 1\n",
      "Fold 1 Epoch 021 | train_loss: 0.18985 | val_loss: 0.31210 | val_smape: 50.3258% | best_smape: 50.3258% | wait: 0\n",
      "Fold 1 Epoch 022 | train_loss: 0.18436 | val_loss: 0.31427 | val_smape: 50.4997% | best_smape: 50.3258% | wait: 1\n",
      "Fold 1 Epoch 023 | train_loss: 0.18143 | val_loss: 0.31678 | val_smape: 50.8335% | best_smape: 50.3258% | wait: 2\n",
      "Fold 1 Epoch 024 | train_loss: 0.17773 | val_loss: 0.31304 | val_smape: 50.3564% | best_smape: 50.3258% | wait: 3\n",
      "Fold 1 Epoch 025 | train_loss: 0.17588 | val_loss: 0.31382 | val_smape: 50.4699% | best_smape: 50.3258% | wait: 4\n",
      "Fold 1 Epoch 026 | train_loss: 0.17347 | val_loss: 0.31260 | val_smape: 50.2926% | best_smape: 50.3258% | wait: 5\n",
      "Fold 1 Epoch 027 | train_loss: 0.17168 | val_loss: 0.31424 | val_smape: 50.5209% | best_smape: 50.3258% | wait: 6\n",
      "Fold 1 Epoch 028 | train_loss: 0.17091 | val_loss: 0.31369 | val_smape: 50.4493% | best_smape: 50.3258% | wait: 7\n",
      "Fold 1 Epoch 029 | train_loss: 0.17029 | val_loss: 0.31269 | val_smape: 50.3120% | best_smape: 50.3258% | wait: 8\n",
      "Fold 1 Epoch 030 | train_loss: 0.18317 | val_loss: 0.31556 | val_smape: 50.6347% | best_smape: 50.3258% | wait: 9\n",
      "Fold 1 Epoch 031 | train_loss: 0.18149 | val_loss: 0.31880 | val_smape: 51.0676% | best_smape: 50.3258% | wait: 10\n",
      "Fold 1 Epoch 032 | train_loss: 0.17625 | val_loss: 0.31740 | val_smape: 50.8141% | best_smape: 50.3258% | wait: 11\n",
      "Fold 1 Epoch 033 | train_loss: 0.17131 | val_loss: 0.31292 | val_smape: 50.3505% | best_smape: 50.3258% | wait: 12\n",
      "Early stopping (wait >= 12) at epoch 33\n",
      "Saved fold model to models/fold_model_1.pth\n",
      "Fold 1 SMAPE: 50.3258%\n",
      "\n",
      "================================================================================\n",
      "Starting Fold 2/5\n",
      "Fold 2 Epoch 001 | train_loss: 0.52563 | val_loss: 0.40549 | val_smape: 62.2764% | best_smape: 62.2764% | wait: 0\n",
      "Fold 2 Epoch 002 | train_loss: 0.40538 | val_loss: 0.37029 | val_smape: 58.0993% | best_smape: 58.0993% | wait: 0\n",
      "Fold 2 Epoch 003 | train_loss: 0.37196 | val_loss: 0.35048 | val_smape: 55.6298% | best_smape: 55.6298% | wait: 0\n",
      "Fold 2 Epoch 004 | train_loss: 0.35063 | val_loss: 0.33833 | val_smape: 54.1020% | best_smape: 54.1020% | wait: 0\n",
      "Fold 2 Epoch 005 | train_loss: 0.33260 | val_loss: 0.33963 | val_smape: 54.2980% | best_smape: 54.1020% | wait: 1\n",
      "Fold 2 Epoch 006 | train_loss: 0.31379 | val_loss: 0.32398 | val_smape: 52.1413% | best_smape: 52.1413% | wait: 0\n",
      "Fold 2 Epoch 007 | train_loss: 0.30455 | val_loss: 0.32530 | val_smape: 52.3429% | best_smape: 52.1413% | wait: 1\n",
      "Fold 2 Epoch 008 | train_loss: 0.29856 | val_loss: 0.32254 | val_smape: 51.9773% | best_smape: 51.9773% | wait: 0\n",
      "Fold 2 Epoch 009 | train_loss: 0.29696 | val_loss: 0.32265 | val_smape: 52.0121% | best_smape: 51.9773% | wait: 1\n",
      "Fold 2 Epoch 010 | train_loss: 0.30223 | val_loss: 0.32055 | val_smape: 51.6992% | best_smape: 51.6992% | wait: 0\n",
      "Fold 2 Epoch 011 | train_loss: 0.28800 | val_loss: 0.31719 | val_smape: 51.1964% | best_smape: 51.1964% | wait: 0\n",
      "Fold 2 Epoch 012 | train_loss: 0.27255 | val_loss: 0.31724 | val_smape: 51.1539% | best_smape: 51.1964% | wait: 1\n",
      "Fold 2 Epoch 013 | train_loss: 0.25898 | val_loss: 0.31428 | val_smape: 50.8218% | best_smape: 50.8218% | wait: 0\n",
      "Fold 2 Epoch 014 | train_loss: 0.24714 | val_loss: 0.31162 | val_smape: 50.3659% | best_smape: 50.3659% | wait: 0\n",
      "Fold 2 Epoch 015 | train_loss: 0.23485 | val_loss: 0.31597 | val_smape: 50.9626% | best_smape: 50.3659% | wait: 1\n",
      "Fold 2 Epoch 016 | train_loss: 0.22453 | val_loss: 0.31071 | val_smape: 50.1701% | best_smape: 50.1701% | wait: 0\n",
      "Fold 2 Epoch 017 | train_loss: 0.21611 | val_loss: 0.31364 | val_smape: 50.5689% | best_smape: 50.1701% | wait: 1\n",
      "Fold 2 Epoch 018 | train_loss: 0.20668 | val_loss: 0.31046 | val_smape: 50.0155% | best_smape: 50.0155% | wait: 0\n",
      "Fold 2 Epoch 019 | train_loss: 0.19998 | val_loss: 0.31032 | val_smape: 50.0447% | best_smape: 50.0155% | wait: 1\n",
      "Fold 2 Epoch 020 | train_loss: 0.19431 | val_loss: 0.31098 | val_smape: 50.1044% | best_smape: 50.0155% | wait: 2\n",
      "Fold 2 Epoch 021 | train_loss: 0.18744 | val_loss: 0.31166 | val_smape: 50.0723% | best_smape: 50.0155% | wait: 3\n",
      "Fold 2 Epoch 022 | train_loss: 0.18328 | val_loss: 0.30807 | val_smape: 49.6867% | best_smape: 49.6867% | wait: 0\n",
      "Fold 2 Epoch 023 | train_loss: 0.17910 | val_loss: 0.30849 | val_smape: 49.7140% | best_smape: 49.6867% | wait: 1\n",
      "Fold 2 Epoch 024 | train_loss: 0.17543 | val_loss: 0.31403 | val_smape: 50.3795% | best_smape: 49.6867% | wait: 2\n",
      "Fold 2 Epoch 025 | train_loss: 0.17235 | val_loss: 0.30980 | val_smape: 49.8114% | best_smape: 49.6867% | wait: 3\n",
      "Fold 2 Epoch 026 | train_loss: 0.17091 | val_loss: 0.31073 | val_smape: 49.9748% | best_smape: 49.6867% | wait: 4\n",
      "Fold 2 Epoch 027 | train_loss: 0.17073 | val_loss: 0.31018 | val_smape: 49.8828% | best_smape: 49.6867% | wait: 5\n",
      "Fold 2 Epoch 028 | train_loss: 0.16898 | val_loss: 0.31028 | val_smape: 49.8739% | best_smape: 49.6867% | wait: 6\n",
      "Fold 2 Epoch 029 | train_loss: 0.16964 | val_loss: 0.30984 | val_smape: 49.8306% | best_smape: 49.6867% | wait: 7\n",
      "Fold 2 Epoch 030 | train_loss: 0.18168 | val_loss: 0.31249 | val_smape: 50.1787% | best_smape: 49.6867% | wait: 8\n",
      "Fold 2 Epoch 031 | train_loss: 0.17924 | val_loss: 0.31683 | val_smape: 50.7620% | best_smape: 49.6867% | wait: 9\n",
      "Fold 2 Epoch 032 | train_loss: 0.17545 | val_loss: 0.31078 | val_smape: 49.9743% | best_smape: 49.6867% | wait: 10\n",
      "Fold 2 Epoch 033 | train_loss: 0.17043 | val_loss: 0.31344 | val_smape: 50.2743% | best_smape: 49.6867% | wait: 11\n",
      "Fold 2 Epoch 034 | train_loss: 0.16588 | val_loss: 0.31442 | val_smape: 50.2405% | best_smape: 49.6867% | wait: 12\n",
      "Early stopping (wait >= 12) at epoch 34\n",
      "Saved fold model to models/fold_model_2.pth\n",
      "Fold 2 SMAPE: 49.6867%\n",
      "\n",
      "================================================================================\n",
      "Starting Fold 3/5\n",
      "Fold 3 Epoch 001 | train_loss: 0.56264 | val_loss: 0.40021 | val_smape: 61.6358% | best_smape: 61.6358% | wait: 0\n",
      "Fold 3 Epoch 002 | train_loss: 0.40901 | val_loss: 0.36698 | val_smape: 57.8603% | best_smape: 57.8603% | wait: 0\n",
      "Fold 3 Epoch 003 | train_loss: 0.37670 | val_loss: 0.34875 | val_smape: 55.5526% | best_smape: 55.5526% | wait: 0\n",
      "Fold 3 Epoch 004 | train_loss: 0.35303 | val_loss: 0.33833 | val_smape: 54.3482% | best_smape: 54.3482% | wait: 0\n",
      "Fold 3 Epoch 005 | train_loss: 0.33568 | val_loss: 0.32558 | val_smape: 52.7022% | best_smape: 52.7022% | wait: 0\n",
      "Fold 3 Epoch 006 | train_loss: 0.31448 | val_loss: 0.32239 | val_smape: 52.2380% | best_smape: 52.2380% | wait: 0\n",
      "Fold 3 Epoch 007 | train_loss: 0.30539 | val_loss: 0.32000 | val_smape: 51.9435% | best_smape: 51.9435% | wait: 0\n",
      "Fold 3 Epoch 008 | train_loss: 0.30096 | val_loss: 0.32027 | val_smape: 51.9713% | best_smape: 51.9435% | wait: 1\n",
      "Fold 3 Epoch 009 | train_loss: 0.29729 | val_loss: 0.31984 | val_smape: 51.8775% | best_smape: 51.8775% | wait: 0\n",
      "Fold 3 Epoch 010 | train_loss: 0.30526 | val_loss: 0.31840 | val_smape: 51.8049% | best_smape: 51.8049% | wait: 0\n",
      "Fold 3 Epoch 011 | train_loss: 0.28884 | val_loss: 0.31320 | val_smape: 50.9940% | best_smape: 50.9940% | wait: 0\n",
      "Fold 3 Epoch 012 | train_loss: 0.27422 | val_loss: 0.31457 | val_smape: 51.0453% | best_smape: 50.9940% | wait: 1\n",
      "Fold 3 Epoch 013 | train_loss: 0.26010 | val_loss: 0.30856 | val_smape: 50.1771% | best_smape: 50.1771% | wait: 0\n",
      "Fold 3 Epoch 014 | train_loss: 0.24909 | val_loss: 0.31098 | val_smape: 50.4013% | best_smape: 50.1771% | wait: 1\n",
      "Fold 3 Epoch 015 | train_loss: 0.23691 | val_loss: 0.30798 | val_smape: 50.0568% | best_smape: 50.0568% | wait: 0\n",
      "Fold 3 Epoch 016 | train_loss: 0.22703 | val_loss: 0.30681 | val_smape: 49.8296% | best_smape: 49.8296% | wait: 0\n",
      "Fold 3 Epoch 017 | train_loss: 0.21758 | val_loss: 0.30882 | val_smape: 49.9665% | best_smape: 49.8296% | wait: 1\n",
      "Fold 3 Epoch 018 | train_loss: 0.20905 | val_loss: 0.30683 | val_smape: 49.7008% | best_smape: 49.7008% | wait: 0\n",
      "Fold 3 Epoch 019 | train_loss: 0.20108 | val_loss: 0.30689 | val_smape: 49.6185% | best_smape: 49.6185% | wait: 0\n",
      "Fold 3 Epoch 020 | train_loss: 0.19527 | val_loss: 0.31551 | val_smape: 50.8084% | best_smape: 49.6185% | wait: 1\n",
      "Fold 3 Epoch 021 | train_loss: 0.18945 | val_loss: 0.31015 | val_smape: 49.9963% | best_smape: 49.6185% | wait: 2\n",
      "Fold 3 Epoch 022 | train_loss: 0.18451 | val_loss: 0.30763 | val_smape: 49.6887% | best_smape: 49.6185% | wait: 3\n",
      "Fold 3 Epoch 023 | train_loss: 0.18078 | val_loss: 0.30798 | val_smape: 49.8008% | best_smape: 49.6185% | wait: 4\n",
      "Fold 3 Epoch 024 | train_loss: 0.17753 | val_loss: 0.30890 | val_smape: 49.8311% | best_smape: 49.6185% | wait: 5\n",
      "Fold 3 Epoch 025 | train_loss: 0.17369 | val_loss: 0.31080 | val_smape: 50.1172% | best_smape: 49.6185% | wait: 6\n",
      "Fold 3 Epoch 026 | train_loss: 0.17383 | val_loss: 0.30700 | val_smape: 49.5682% | best_smape: 49.5682% | wait: 0\n",
      "Fold 3 Epoch 027 | train_loss: 0.17133 | val_loss: 0.30737 | val_smape: 49.6257% | best_smape: 49.5682% | wait: 1\n",
      "Fold 3 Epoch 028 | train_loss: 0.17145 | val_loss: 0.30789 | val_smape: 49.6685% | best_smape: 49.5682% | wait: 2\n",
      "Fold 3 Epoch 029 | train_loss: 0.16991 | val_loss: 0.30804 | val_smape: 49.6966% | best_smape: 49.5682% | wait: 3\n",
      "Fold 3 Epoch 030 | train_loss: 0.18394 | val_loss: 0.31002 | val_smape: 50.0972% | best_smape: 49.5682% | wait: 4\n",
      "Fold 3 Epoch 031 | train_loss: 0.18109 | val_loss: 0.30737 | val_smape: 49.5514% | best_smape: 49.5682% | wait: 5\n",
      "Fold 3 Epoch 032 | train_loss: 0.17661 | val_loss: 0.31270 | val_smape: 50.4071% | best_smape: 49.5682% | wait: 6\n",
      "Fold 3 Epoch 033 | train_loss: 0.17096 | val_loss: 0.30872 | val_smape: 49.8889% | best_smape: 49.5682% | wait: 7\n",
      "Fold 3 Epoch 034 | train_loss: 0.16712 | val_loss: 0.30878 | val_smape: 49.7725% | best_smape: 49.5682% | wait: 8\n",
      "Fold 3 Epoch 035 | train_loss: 0.16318 | val_loss: 0.31302 | val_smape: 50.3138% | best_smape: 49.5682% | wait: 9\n",
      "Fold 3 Epoch 036 | train_loss: 0.15990 | val_loss: 0.30667 | val_smape: 49.4174% | best_smape: 49.4174% | wait: 0\n",
      "Fold 3 Epoch 037 | train_loss: 0.15558 | val_loss: 0.31081 | val_smape: 50.0235% | best_smape: 49.4174% | wait: 1\n",
      "Fold 3 Epoch 038 | train_loss: 0.15121 | val_loss: 0.30616 | val_smape: 49.3721% | best_smape: 49.4174% | wait: 2\n",
      "Fold 3 Epoch 039 | train_loss: 0.14851 | val_loss: 0.30615 | val_smape: 49.4711% | best_smape: 49.4174% | wait: 3\n",
      "Fold 3 Epoch 040 | train_loss: 0.14503 | val_loss: 0.30631 | val_smape: 49.2902% | best_smape: 49.2902% | wait: 0\n",
      "Fold 3 Epoch 041 | train_loss: 0.14360 | val_loss: 0.30395 | val_smape: 48.9992% | best_smape: 48.9992% | wait: 0\n",
      "Fold 3 Epoch 042 | train_loss: 0.13874 | val_loss: 0.30473 | val_smape: 49.1313% | best_smape: 48.9992% | wait: 1\n",
      "Fold 3 Epoch 043 | train_loss: 0.13635 | val_loss: 0.30420 | val_smape: 48.9702% | best_smape: 48.9992% | wait: 2\n",
      "Fold 3 Epoch 044 | train_loss: 0.13380 | val_loss: 0.30881 | val_smape: 49.6313% | best_smape: 48.9992% | wait: 3\n",
      "Fold 3 Epoch 045 | train_loss: 0.13115 | val_loss: 0.30851 | val_smape: 49.6290% | best_smape: 48.9992% | wait: 4\n",
      "Fold 3 Epoch 046 | train_loss: 0.12828 | val_loss: 0.30593 | val_smape: 49.2366% | best_smape: 48.9992% | wait: 5\n",
      "Fold 3 Epoch 047 | train_loss: 0.12724 | val_loss: 0.30584 | val_smape: 49.1740% | best_smape: 48.9992% | wait: 6\n",
      "Fold 3 Epoch 048 | train_loss: 0.12485 | val_loss: 0.30760 | val_smape: 49.4387% | best_smape: 48.9992% | wait: 7\n",
      "Fold 3 Epoch 049 | train_loss: 0.12302 | val_loss: 0.30966 | val_smape: 49.6700% | best_smape: 48.9992% | wait: 8\n",
      "Fold 3 Epoch 050 | train_loss: 0.12095 | val_loss: 0.30375 | val_smape: 48.9481% | best_smape: 48.9481% | wait: 0\n",
      "Fold 3 Epoch 051 | train_loss: 0.11932 | val_loss: 0.30570 | val_smape: 49.2291% | best_smape: 48.9481% | wait: 1\n",
      "Fold 3 Epoch 052 | train_loss: 0.11765 | val_loss: 0.30505 | val_smape: 49.0892% | best_smape: 48.9481% | wait: 2\n",
      "Fold 3 Epoch 053 | train_loss: 0.11630 | val_loss: 0.30402 | val_smape: 48.9874% | best_smape: 48.9481% | wait: 3\n",
      "Fold 3 Epoch 054 | train_loss: 0.11518 | val_loss: 0.30293 | val_smape: 48.9153% | best_smape: 48.9481% | wait: 4\n",
      "Fold 3 Epoch 055 | train_loss: 0.11335 | val_loss: 0.30470 | val_smape: 49.1001% | best_smape: 48.9481% | wait: 5\n",
      "Fold 3 Epoch 056 | train_loss: 0.11329 | val_loss: 0.30348 | val_smape: 48.8750% | best_smape: 48.8750% | wait: 0\n",
      "Fold 3 Epoch 057 | train_loss: 0.11253 | val_loss: 0.30466 | val_smape: 49.0899% | best_smape: 48.8750% | wait: 1\n",
      "Fold 3 Epoch 058 | train_loss: 0.11078 | val_loss: 0.30431 | val_smape: 48.9300% | best_smape: 48.8750% | wait: 2\n",
      "Fold 3 Epoch 059 | train_loss: 0.10935 | val_loss: 0.30351 | val_smape: 48.9176% | best_smape: 48.8750% | wait: 3\n",
      "Fold 3 Epoch 060 | train_loss: 0.10853 | val_loss: 0.30314 | val_smape: 48.8283% | best_smape: 48.8750% | wait: 4\n",
      "Fold 3 Epoch 061 | train_loss: 0.10884 | val_loss: 0.30300 | val_smape: 48.8235% | best_smape: 48.8235% | wait: 0\n",
      "Fold 3 Epoch 062 | train_loss: 0.10813 | val_loss: 0.30357 | val_smape: 48.8747% | best_smape: 48.8235% | wait: 1\n",
      "Fold 3 Epoch 063 | train_loss: 0.10728 | val_loss: 0.30341 | val_smape: 48.8715% | best_smape: 48.8235% | wait: 2\n",
      "Fold 3 Epoch 064 | train_loss: 0.10624 | val_loss: 0.30230 | val_smape: 48.7443% | best_smape: 48.7443% | wait: 0\n",
      "Fold 3 Epoch 065 | train_loss: 0.10675 | val_loss: 0.30334 | val_smape: 48.8419% | best_smape: 48.7443% | wait: 1\n",
      "Fold 3 Epoch 066 | train_loss: 0.10659 | val_loss: 0.30288 | val_smape: 48.8079% | best_smape: 48.7443% | wait: 2\n",
      "Fold 3 Epoch 067 | train_loss: 0.10596 | val_loss: 0.30351 | val_smape: 48.8923% | best_smape: 48.7443% | wait: 3\n",
      "Fold 3 Epoch 068 | train_loss: 0.10662 | val_loss: 0.30327 | val_smape: 48.8563% | best_smape: 48.7443% | wait: 4\n",
      "Fold 3 Epoch 069 | train_loss: 0.10571 | val_loss: 0.30317 | val_smape: 48.8429% | best_smape: 48.7443% | wait: 5\n",
      "Fold 3 Epoch 070 | train_loss: 0.11630 | val_loss: 0.30711 | val_smape: 49.3093% | best_smape: 48.7443% | wait: 6\n",
      "Fold 3 Epoch 071 | train_loss: 0.11894 | val_loss: 0.30644 | val_smape: 49.1895% | best_smape: 48.7443% | wait: 7\n",
      "Fold 3 Epoch 072 | train_loss: 0.11882 | val_loss: 0.30562 | val_smape: 49.1759% | best_smape: 48.7443% | wait: 8\n",
      "Fold 3 Epoch 073 | train_loss: 0.11727 | val_loss: 0.31062 | val_smape: 49.7836% | best_smape: 48.7443% | wait: 9\n",
      "Fold 3 Epoch 074 | train_loss: 0.11608 | val_loss: 0.30867 | val_smape: 49.5470% | best_smape: 48.7443% | wait: 10\n",
      "Fold 3 Epoch 075 | train_loss: 0.11579 | val_loss: 0.30682 | val_smape: 49.2178% | best_smape: 48.7443% | wait: 11\n",
      "Fold 3 Epoch 076 | train_loss: 0.11382 | val_loss: 0.30443 | val_smape: 49.0343% | best_smape: 48.7443% | wait: 12\n",
      "Early stopping (wait >= 12) at epoch 76\n",
      "Saved fold model to models/fold_model_3.pth\n",
      "Fold 3 SMAPE: 48.7443%\n",
      "\n",
      "================================================================================\n",
      "Starting Fold 4/5\n",
      "Fold 4 Epoch 001 | train_loss: 0.55260 | val_loss: 0.39813 | val_smape: 61.3549% | best_smape: 61.3549% | wait: 0\n",
      "Fold 4 Epoch 002 | train_loss: 0.40819 | val_loss: 0.35776 | val_smape: 56.4984% | best_smape: 56.4984% | wait: 0\n",
      "Fold 4 Epoch 003 | train_loss: 0.37470 | val_loss: 0.34056 | val_smape: 54.4823% | best_smape: 54.4823% | wait: 0\n",
      "Fold 4 Epoch 004 | train_loss: 0.35333 | val_loss: 0.33247 | val_smape: 53.4247% | best_smape: 53.4247% | wait: 0\n",
      "Fold 4 Epoch 005 | train_loss: 0.33519 | val_loss: 0.32466 | val_smape: 52.4256% | best_smape: 52.4256% | wait: 0\n",
      "Fold 4 Epoch 006 | train_loss: 0.31425 | val_loss: 0.31589 | val_smape: 51.2159% | best_smape: 51.2159% | wait: 0\n",
      "Fold 4 Epoch 007 | train_loss: 0.30629 | val_loss: 0.31836 | val_smape: 51.5588% | best_smape: 51.2159% | wait: 1\n",
      "Fold 4 Epoch 008 | train_loss: 0.30007 | val_loss: 0.31709 | val_smape: 51.3917% | best_smape: 51.2159% | wait: 2\n",
      "Fold 4 Epoch 009 | train_loss: 0.29710 | val_loss: 0.31675 | val_smape: 51.3049% | best_smape: 51.2159% | wait: 3\n",
      "Fold 4 Epoch 010 | train_loss: 0.30407 | val_loss: 0.31367 | val_smape: 50.7962% | best_smape: 50.7962% | wait: 0\n",
      "Fold 4 Epoch 011 | train_loss: 0.28725 | val_loss: 0.31716 | val_smape: 51.2614% | best_smape: 50.7962% | wait: 1\n",
      "Fold 4 Epoch 012 | train_loss: 0.27395 | val_loss: 0.30966 | val_smape: 50.2016% | best_smape: 50.2016% | wait: 0\n",
      "Fold 4 Epoch 013 | train_loss: 0.26052 | val_loss: 0.31477 | val_smape: 50.9402% | best_smape: 50.2016% | wait: 1\n",
      "Fold 4 Epoch 014 | train_loss: 0.24828 | val_loss: 0.30592 | val_smape: 49.6906% | best_smape: 49.6906% | wait: 0\n",
      "Fold 4 Epoch 015 | train_loss: 0.23642 | val_loss: 0.30697 | val_smape: 49.6470% | best_smape: 49.6906% | wait: 1\n",
      "Fold 4 Epoch 016 | train_loss: 0.22545 | val_loss: 0.30556 | val_smape: 49.5856% | best_smape: 49.5856% | wait: 0\n",
      "Fold 4 Epoch 017 | train_loss: 0.21753 | val_loss: 0.30649 | val_smape: 49.5725% | best_smape: 49.5856% | wait: 1\n",
      "Fold 4 Epoch 018 | train_loss: 0.20861 | val_loss: 0.30901 | val_smape: 49.8999% | best_smape: 49.5856% | wait: 2\n",
      "Fold 4 Epoch 019 | train_loss: 0.20050 | val_loss: 0.30776 | val_smape: 49.6963% | best_smape: 49.5856% | wait: 3\n",
      "Fold 4 Epoch 020 | train_loss: 0.19497 | val_loss: 0.30969 | val_smape: 49.9559% | best_smape: 49.5856% | wait: 4\n",
      "Fold 4 Epoch 021 | train_loss: 0.18792 | val_loss: 0.31161 | val_smape: 50.2354% | best_smape: 49.5856% | wait: 5\n",
      "Fold 4 Epoch 022 | train_loss: 0.18401 | val_loss: 0.30987 | val_smape: 49.9774% | best_smape: 49.5856% | wait: 6\n",
      "Fold 4 Epoch 023 | train_loss: 0.17998 | val_loss: 0.30513 | val_smape: 49.3755% | best_smape: 49.3755% | wait: 0\n",
      "Fold 4 Epoch 024 | train_loss: 0.17613 | val_loss: 0.30848 | val_smape: 49.8000% | best_smape: 49.3755% | wait: 1\n",
      "Fold 4 Epoch 025 | train_loss: 0.17408 | val_loss: 0.30864 | val_smape: 49.8298% | best_smape: 49.3755% | wait: 2\n",
      "Fold 4 Epoch 026 | train_loss: 0.17122 | val_loss: 0.30753 | val_smape: 49.6325% | best_smape: 49.3755% | wait: 3\n",
      "Fold 4 Epoch 027 | train_loss: 0.16983 | val_loss: 0.30707 | val_smape: 49.5733% | best_smape: 49.3755% | wait: 4\n",
      "Fold 4 Epoch 028 | train_loss: 0.16895 | val_loss: 0.30909 | val_smape: 49.8323% | best_smape: 49.3755% | wait: 5\n",
      "Fold 4 Epoch 029 | train_loss: 0.16916 | val_loss: 0.30866 | val_smape: 49.7820% | best_smape: 49.3755% | wait: 6\n",
      "Fold 4 Epoch 030 | train_loss: 0.18225 | val_loss: 0.31197 | val_smape: 50.1429% | best_smape: 49.3755% | wait: 7\n",
      "Fold 4 Epoch 031 | train_loss: 0.18049 | val_loss: 0.31182 | val_smape: 50.1225% | best_smape: 49.3755% | wait: 8\n",
      "Fold 4 Epoch 032 | train_loss: 0.17516 | val_loss: 0.31019 | val_smape: 49.9914% | best_smape: 49.3755% | wait: 9\n",
      "Fold 4 Epoch 033 | train_loss: 0.17072 | val_loss: 0.30755 | val_smape: 49.6056% | best_smape: 49.3755% | wait: 10\n",
      "Fold 4 Epoch 034 | train_loss: 0.16600 | val_loss: 0.30840 | val_smape: 49.8000% | best_smape: 49.3755% | wait: 11\n",
      "Fold 4 Epoch 035 | train_loss: 0.16105 | val_loss: 0.30771 | val_smape: 49.6371% | best_smape: 49.3755% | wait: 12\n",
      "Early stopping (wait >= 12) at epoch 35\n",
      "Saved fold model to models/fold_model_4.pth\n",
      "Fold 4 SMAPE: 49.3755%\n",
      "\n",
      "================================================================================\n",
      "Starting Fold 5/5\n",
      "Fold 5 Epoch 001 | train_loss: 0.53860 | val_loss: 0.39788 | val_smape: 61.2461% | best_smape: 61.2461% | wait: 0\n",
      "Fold 5 Epoch 002 | train_loss: 0.40412 | val_loss: 0.37197 | val_smape: 58.2072% | best_smape: 58.2072% | wait: 0\n",
      "Fold 5 Epoch 003 | train_loss: 0.37236 | val_loss: 0.34938 | val_smape: 55.4769% | best_smape: 55.4769% | wait: 0\n",
      "Fold 5 Epoch 004 | train_loss: 0.35173 | val_loss: 0.34297 | val_smape: 54.6851% | best_smape: 54.6851% | wait: 0\n",
      "Fold 5 Epoch 005 | train_loss: 0.33178 | val_loss: 0.32479 | val_smape: 52.3758% | best_smape: 52.3758% | wait: 0\n",
      "Fold 5 Epoch 006 | train_loss: 0.31190 | val_loss: 0.32323 | val_smape: 52.0868% | best_smape: 52.0868% | wait: 0\n",
      "Fold 5 Epoch 007 | train_loss: 0.30375 | val_loss: 0.32016 | val_smape: 51.6337% | best_smape: 51.6337% | wait: 0\n",
      "Fold 5 Epoch 008 | train_loss: 0.29848 | val_loss: 0.32018 | val_smape: 51.6400% | best_smape: 51.6337% | wait: 1\n",
      "Fold 5 Epoch 009 | train_loss: 0.29586 | val_loss: 0.31995 | val_smape: 51.6242% | best_smape: 51.6337% | wait: 2\n",
      "Fold 5 Epoch 010 | train_loss: 0.30165 | val_loss: 0.32307 | val_smape: 52.0539% | best_smape: 51.6337% | wait: 3\n",
      "Fold 5 Epoch 011 | train_loss: 0.28675 | val_loss: 0.32303 | val_smape: 52.0742% | best_smape: 51.6337% | wait: 4\n",
      "Fold 5 Epoch 012 | train_loss: 0.27229 | val_loss: 0.31679 | val_smape: 51.1642% | best_smape: 51.1642% | wait: 0\n",
      "Fold 5 Epoch 013 | train_loss: 0.25971 | val_loss: 0.30938 | val_smape: 50.1476% | best_smape: 50.1476% | wait: 0\n",
      "Fold 5 Epoch 014 | train_loss: 0.24800 | val_loss: 0.31070 | val_smape: 50.2073% | best_smape: 50.1476% | wait: 1\n",
      "Fold 5 Epoch 015 | train_loss: 0.23629 | val_loss: 0.30853 | val_smape: 49.9790% | best_smape: 49.9790% | wait: 0\n",
      "Fold 5 Epoch 016 | train_loss: 0.22497 | val_loss: 0.30973 | val_smape: 49.9624% | best_smape: 49.9790% | wait: 1\n",
      "Fold 5 Epoch 017 | train_loss: 0.21750 | val_loss: 0.30880 | val_smape: 49.8860% | best_smape: 49.8860% | wait: 0\n",
      "Fold 5 Epoch 018 | train_loss: 0.20776 | val_loss: 0.31096 | val_smape: 50.0524% | best_smape: 49.8860% | wait: 1\n",
      "Fold 5 Epoch 019 | train_loss: 0.20028 | val_loss: 0.31370 | val_smape: 50.3854% | best_smape: 49.8860% | wait: 2\n",
      "Fold 5 Epoch 020 | train_loss: 0.19358 | val_loss: 0.31508 | val_smape: 50.6323% | best_smape: 49.8860% | wait: 3\n",
      "Fold 5 Epoch 021 | train_loss: 0.18838 | val_loss: 0.31460 | val_smape: 50.4286% | best_smape: 49.8860% | wait: 4\n",
      "Fold 5 Epoch 022 | train_loss: 0.18412 | val_loss: 0.30969 | val_smape: 49.8320% | best_smape: 49.8320% | wait: 0\n",
      "Fold 5 Epoch 023 | train_loss: 0.18013 | val_loss: 0.31161 | val_smape: 50.0821% | best_smape: 49.8320% | wait: 1\n",
      "Fold 5 Epoch 024 | train_loss: 0.17650 | val_loss: 0.31651 | val_smape: 50.6840% | best_smape: 49.8320% | wait: 2\n",
      "Fold 5 Epoch 025 | train_loss: 0.17382 | val_loss: 0.31323 | val_smape: 50.2495% | best_smape: 49.8320% | wait: 3\n",
      "Fold 5 Epoch 026 | train_loss: 0.17256 | val_loss: 0.30904 | val_smape: 49.7330% | best_smape: 49.7330% | wait: 0\n",
      "Fold 5 Epoch 027 | train_loss: 0.17105 | val_loss: 0.31076 | val_smape: 49.9319% | best_smape: 49.7330% | wait: 1\n",
      "Fold 5 Epoch 028 | train_loss: 0.17005 | val_loss: 0.31183 | val_smape: 50.0495% | best_smape: 49.7330% | wait: 2\n",
      "Fold 5 Epoch 029 | train_loss: 0.16892 | val_loss: 0.31143 | val_smape: 49.9936% | best_smape: 49.7330% | wait: 3\n",
      "Fold 5 Epoch 030 | train_loss: 0.18275 | val_loss: 0.31360 | val_smape: 50.3815% | best_smape: 49.7330% | wait: 4\n",
      "Fold 5 Epoch 031 | train_loss: 0.17968 | val_loss: 0.31169 | val_smape: 49.9436% | best_smape: 49.7330% | wait: 5\n",
      "Fold 5 Epoch 032 | train_loss: 0.17481 | val_loss: 0.30979 | val_smape: 49.6796% | best_smape: 49.6796% | wait: 0\n",
      "Fold 5 Epoch 033 | train_loss: 0.17075 | val_loss: 0.31127 | val_smape: 50.0024% | best_smape: 49.6796% | wait: 1\n",
      "Fold 5 Epoch 034 | train_loss: 0.16639 | val_loss: 0.31317 | val_smape: 50.1911% | best_smape: 49.6796% | wait: 2\n",
      "Fold 5 Epoch 035 | train_loss: 0.16176 | val_loss: 0.31546 | val_smape: 50.4094% | best_smape: 49.6796% | wait: 3\n",
      "Fold 5 Epoch 036 | train_loss: 0.15792 | val_loss: 0.31687 | val_smape: 50.6251% | best_smape: 49.6796% | wait: 4\n",
      "Fold 5 Epoch 037 | train_loss: 0.15411 | val_loss: 0.31484 | val_smape: 50.3052% | best_smape: 49.6796% | wait: 5\n",
      "Fold 5 Epoch 038 | train_loss: 0.15052 | val_loss: 0.31182 | val_smape: 49.9082% | best_smape: 49.6796% | wait: 6\n",
      "Fold 5 Epoch 039 | train_loss: 0.14638 | val_loss: 0.31550 | val_smape: 50.3381% | best_smape: 49.6796% | wait: 7\n",
      "Fold 5 Epoch 040 | train_loss: 0.14380 | val_loss: 0.31187 | val_smape: 49.9797% | best_smape: 49.6796% | wait: 8\n",
      "Fold 5 Epoch 041 | train_loss: 0.14139 | val_loss: 0.30829 | val_smape: 49.5764% | best_smape: 49.5764% | wait: 0\n",
      "Fold 5 Epoch 042 | train_loss: 0.13888 | val_loss: 0.30954 | val_smape: 49.6288% | best_smape: 49.5764% | wait: 1\n",
      "Fold 5 Epoch 043 | train_loss: 0.13638 | val_loss: 0.31150 | val_smape: 49.8108% | best_smape: 49.5764% | wait: 2\n",
      "Fold 5 Epoch 044 | train_loss: 0.13263 | val_loss: 0.31128 | val_smape: 49.8316% | best_smape: 49.5764% | wait: 3\n",
      "Fold 5 Epoch 045 | train_loss: 0.13057 | val_loss: 0.30693 | val_smape: 49.2534% | best_smape: 49.2534% | wait: 0\n",
      "Fold 5 Epoch 046 | train_loss: 0.12888 | val_loss: 0.31296 | val_smape: 50.0040% | best_smape: 49.2534% | wait: 1\n",
      "Fold 5 Epoch 047 | train_loss: 0.12679 | val_loss: 0.31017 | val_smape: 49.7305% | best_smape: 49.2534% | wait: 2\n",
      "Fold 5 Epoch 048 | train_loss: 0.12426 | val_loss: 0.30548 | val_smape: 49.1507% | best_smape: 49.1507% | wait: 0\n",
      "Fold 5 Epoch 049 | train_loss: 0.12282 | val_loss: 0.31032 | val_smape: 49.7998% | best_smape: 49.1507% | wait: 1\n",
      "Fold 5 Epoch 050 | train_loss: 0.12042 | val_loss: 0.31093 | val_smape: 49.7677% | best_smape: 49.1507% | wait: 2\n",
      "Fold 5 Epoch 051 | train_loss: 0.11919 | val_loss: 0.30862 | val_smape: 49.4625% | best_smape: 49.1507% | wait: 3\n",
      "Fold 5 Epoch 052 | train_loss: 0.11828 | val_loss: 0.30969 | val_smape: 49.6766% | best_smape: 49.1507% | wait: 4\n",
      "Fold 5 Epoch 053 | train_loss: 0.11579 | val_loss: 0.30639 | val_smape: 49.2448% | best_smape: 49.1507% | wait: 5\n",
      "Fold 5 Epoch 054 | train_loss: 0.11434 | val_loss: 0.30990 | val_smape: 49.6458% | best_smape: 49.1507% | wait: 6\n",
      "Fold 5 Epoch 055 | train_loss: 0.11370 | val_loss: 0.30639 | val_smape: 49.1998% | best_smape: 49.1507% | wait: 7\n",
      "Fold 5 Epoch 056 | train_loss: 0.11265 | val_loss: 0.30711 | val_smape: 49.2799% | best_smape: 49.1507% | wait: 8\n",
      "Fold 5 Epoch 057 | train_loss: 0.11114 | val_loss: 0.30836 | val_smape: 49.4221% | best_smape: 49.1507% | wait: 9\n",
      "Fold 5 Epoch 058 | train_loss: 0.10999 | val_loss: 0.30741 | val_smape: 49.2945% | best_smape: 49.1507% | wait: 10\n",
      "Fold 5 Epoch 059 | train_loss: 0.10994 | val_loss: 0.30933 | val_smape: 49.6175% | best_smape: 49.1507% | wait: 11\n",
      "Fold 5 Epoch 060 | train_loss: 0.10910 | val_loss: 0.30779 | val_smape: 49.3613% | best_smape: 49.1507% | wait: 12\n",
      "Early stopping (wait >= 12) at epoch 60\n",
      "Saved fold model to models/fold_model_5.pth\n",
      "Fold 5 SMAPE: 49.1507%\n",
      "\n",
      "================================================================================\n",
      "FOLD SMAPEs: [np.float64(50.325808252495975), np.float64(49.686673922855235), np.float64(48.744302622813215), np.float64(49.37550795482748), np.float64(49.150659701366514)]\n",
      "OOF SMAPE: 49.4566%\n",
      "================================================================================\n",
      "Saved submission -> enhanced_mlp_fusion_submission_improved.csv\n",
      "Prediction stats: min 0.45 / mean 21.46 / max 341.14\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Improved Enhanced MLP Fusion (MPS-ready) - Run in one cell\n",
    "# Uses filenames from your code:\n",
    "#   final_X_train_medium_with_brand.npy\n",
    "#   final_X_test_medium_with_brand.npy\n",
    "#   train.csv, test.csv\n",
    "# Output:\n",
    "#   enhanced_mlp_fusion_submission_improved.csv\n",
    "#   saved fold models: fold_model_1.pth ...\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "\n",
    "# ---------------------------\n",
    "# Reproducibility\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ---------------------------\n",
    "# Device\n",
    "# ---------------------------\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ---------------------------\n",
    "# Utility: SMAPE\n",
    "# ---------------------------\n",
    "def smape_np(y_true, y_pred, eps=1e-8):\n",
    "    num = np.abs(y_true - y_pred)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    return np.mean(num / np.maximum(denom, eps)) * 100.0\n",
    "\n",
    "def smape_torch(y_true, y_pred, eps=1e-8):\n",
    "    num = torch.abs(y_true - y_pred)\n",
    "    denom = (torch.abs(y_true) + torch.abs(y_pred)) / 2.0\n",
    "    return torch.mean(num / torch.clamp(denom, min=eps)) * 100.0\n",
    "\n",
    "# ---------------------------\n",
    "# Load data + embeddings (names you used)\n",
    "# ---------------------------\n",
    "print(\"Loading CSVs and embeddings...\")\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test  = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "X_full = np.load(\"embeddings_medium/final_X_train_medium_with_brand.npy\", allow_pickle=False)\n",
    "X_test_full = np.load(\"embeddings_medium/final_X_test_medium_with_brand.npy\", allow_pickle=False)\n",
    "\n",
    "# your code used text_dim=384, image_dim=512\n",
    "TEXT_DIM = 384\n",
    "IMAGE_DIM = 512\n",
    "\n",
    "# slice as you did\n",
    "train_text = X_full[:, :TEXT_DIM]\n",
    "train_image = X_full[:, TEXT_DIM:TEXT_DIM + IMAGE_DIM]\n",
    "train_other_base = X_full[:, TEXT_DIM + IMAGE_DIM:]\n",
    "\n",
    "test_text  = X_test_full[:, :TEXT_DIM]\n",
    "test_image = X_test_full[:, TEXT_DIM:TEXT_DIM + IMAGE_DIM]\n",
    "test_other_base = X_test_full[:, TEXT_DIM + IMAGE_DIM:]\n",
    "\n",
    "del X_full, X_test_full\n",
    "gc.collect()\n",
    "\n",
    "# Extra engineered features from your code may already be in train_other_base; \n",
    "# if not, you can compute additional ones and append. Here we follow your pipeline:\n",
    "def extract_advanced_features(df, is_train=True):\n",
    "    features = {}\n",
    "    txt = df['catalog_content'].astype(str)\n",
    "    features['title_len'] = txt.str.len().fillna(0).values\n",
    "    features['word_count'] = txt.str.split().str.len().fillna(0).values\n",
    "    features['avg_word_len'] = features['title_len'] / (features['word_count'] + 1)\n",
    "    features['has_digits'] = txt.str.contains(r'\\d', regex=True).astype(int).values\n",
    "    features['has_special_chars'] = txt.str.contains(r'[^a-zA-Z0-9\\s]', regex=True).astype(int).values\n",
    "    features['num_numbers'] = txt.str.findall(r'\\d+').str.len().fillna(0).values\n",
    "    # basic keywords\n",
    "    price_keywords = ['premium', 'luxury', 'pro', 'plus', 'max', 'ultra', 'deluxe']\n",
    "    budget_keywords = ['basic', 'mini', 'lite', 'eco', 'value']\n",
    "    features['has_premium_word'] = txt.str.lower().str.contains('|'.join(price_keywords)).astype(int).values\n",
    "    features['has_budget_word'] = txt.str.lower().str.contains('|'.join(budget_keywords)).astype(int).values\n",
    "    return np.vstack([features[k] for k in sorted(features.keys())]).T, list(sorted(features.keys()))\n",
    "\n",
    "train_extra, train_extra_cols = extract_advanced_features(df_train, is_train=True)\n",
    "test_extra, test_extra_cols   = extract_advanced_features(df_test, is_train=False)\n",
    "\n",
    "# Combine \"other\" features\n",
    "train_other = np.hstack([train_other_base, train_extra])\n",
    "test_other  = np.hstack([test_other_base, test_extra])\n",
    "\n",
    "print(\"train_text\", train_text.shape, \"train_image\", train_image.shape, \"train_other\", train_other.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# Scale features (robust)\n",
    "# ---------------------------\n",
    "print(\"Scaling features...\")\n",
    "text_scaler = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=SEED)\n",
    "img_scaler  = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=SEED+1)\n",
    "other_scaler = RobustScaler()\n",
    "\n",
    "train_text_s = text_scaler.fit_transform(train_text)\n",
    "test_text_s  = text_scaler.transform(test_text)\n",
    "\n",
    "train_img_s = img_scaler.fit_transform(train_image)\n",
    "test_img_s  = img_scaler.transform(test_image)\n",
    "\n",
    "train_other_s = other_scaler.fit_transform(train_other)\n",
    "test_other_s  = other_scaler.transform(test_other)\n",
    "\n",
    "del train_text, train_image, train_other, test_text, test_image, test_other\n",
    "gc.collect()\n",
    "\n",
    "# ---------------------------\n",
    "# Target\n",
    "# ---------------------------\n",
    "y = df_train['price'].values  # original prices\n",
    "y_log = np.log1p(y)           # train on log space as you did\n",
    "\n",
    "# ---------------------------\n",
    "# Model definition (refined and lighter)\n",
    "# ---------------------------\n",
    "class FusionMLP(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim, other_dim, hidden=512, dropout=0.18):\n",
    "        super().__init__()\n",
    "        # small per-modality encoders\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(text_dim, hidden//2),\n",
    "            nn.LayerNorm(hidden//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.img_proj = nn.Sequential(\n",
    "            nn.Linear(image_dim, hidden//2),\n",
    "            nn.LayerNorm(hidden//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.other_proj = nn.Sequential(\n",
    "            nn.Linear(other_dim, hidden//4),\n",
    "            nn.LayerNorm(hidden//4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout*0.7),\n",
    "            nn.Linear(hidden//4, hidden//4),\n",
    "            nn.LayerNorm(hidden//4),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        fusion_dim = hidden//2 + hidden//2 + hidden//4\n",
    "        self.gate = nn.Sequential(nn.Linear(fusion_dim, fusion_dim), nn.Sigmoid())\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, hidden),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.LayerNorm(hidden//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout*0.7)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden//2, 1)\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, t, i, o):\n",
    "        t_enc = self.text_proj(t)\n",
    "        i_enc = self.img_proj(i)\n",
    "        o_enc = self.other_proj(o)\n",
    "        fused = torch.cat([t_enc, i_enc, o_enc], dim=1)\n",
    "        gated = self.gate(fused) * fused\n",
    "        x = self.fuse(gated)\n",
    "        return self.out(x)\n",
    "\n",
    "# ---------------------------\n",
    "# Training helpers\n",
    "# ---------------------------\n",
    "def train_one_epoch(model, loader, optimizer, device, loss_fn, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for t, img, oth, yb in loader:\n",
    "        t = t.to(device); img = img.to(device); oth = oth.to(device); yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(t, img, oth)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * t.shape[0]\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def valid_one_epoch(model, loader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for t, img, oth, yb in loader:\n",
    "            t = t.to(device); img = img.to(device); oth = oth.to(device); yb = yb.to(device)\n",
    "            out = model(t, img, oth)\n",
    "            total_loss += loss_fn(out, yb).item() * t.shape[0]\n",
    "            preds.append(out.cpu().numpy())\n",
    "            trues.append(yb.cpu().numpy())\n",
    "    preds = np.concatenate([p.reshape(-1) for p in preds])\n",
    "    trues = np.concatenate([t.reshape(-1) for t in trues])\n",
    "\n",
    "    return total_loss / (len(loader.dataset)), preds, trues\n",
    "\n",
    "# Combined loss: SmoothL1 in log space + small SMAPE penalty (torch)\n",
    "def combined_loss_torch(pred, target, alpha=0.6):\n",
    "    pred = pred.contiguous().view_as(target)\n",
    "    # pred, target are in log-space\n",
    "    huber = nn.SmoothL1Loss()(pred, target)\n",
    "    # compute small smape penalty in log space (using exponentials)\n",
    "    pred_exp = torch.exp(pred)\n",
    "    target_exp = torch.exp(target)\n",
    "    smape_term = torch.mean(torch.abs(pred_exp - target_exp) / ( (torch.abs(pred_exp)+torch.abs(target_exp))/2.0 + 1e-8 ))\n",
    "    return alpha * huber + (1 - alpha) * smape_term\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop with warmup + cosine restarts + early stopping\n",
    "# ---------------------------\n",
    "def fit_fold(X_text, X_img, X_other, y_log, X_text_val, X_img_val, X_other_val, y_log_val,\n",
    "             fold_id, device, model_dir=\"models\", epochs=80, batch_size=256, lr=3e-4):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model = FusionMLP(X_text.shape[1], X_img.shape[1], X_other.shape[1], hidden=512, dropout=0.18).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    # warmup for 5 epochs linearly then cosine restarts\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "    # We'll implement a manual warmup multiplier\n",
    "    warmup_epochs = 5\n",
    "\n",
    "    train_ds = TensorDataset(torch.FloatTensor(X_text), torch.FloatTensor(X_img), torch.FloatTensor(X_other), torch.FloatTensor(y_log))\n",
    "    val_ds   = TensorDataset(torch.FloatTensor(X_text_val), torch.FloatTensor(X_img_val), torch.FloatTensor(X_other_val), torch.FloatTensor(y_log_val))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    best_score = 1e9\n",
    "    best_state = None\n",
    "    patience = 12\n",
    "    wait = 0\n",
    "    # relative-improvement threshold (small)\n",
    "    rel_improve = 0.001\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # manual warmup scale\n",
    "        if epoch <= warmup_epochs:\n",
    "            warmup_scale = epoch / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * warmup_scale\n",
    "        else:\n",
    "            # scheduler step\n",
    "            scheduler.step(epoch + fold_id * 0.0001)  # small offset to differ folds\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device, combined_loss_torch)\n",
    "        val_loss, val_preds_log, val_trues_log = valid_one_epoch(model, val_loader, device, combined_loss_torch)\n",
    "\n",
    "        # compute SMAPE on raw prices\n",
    "        val_preds_price = np.expm1(val_preds_log)\n",
    "        val_trues_price = np.expm1(val_trues_log)\n",
    "        val_smape = smape_np(val_trues_price, val_preds_price)\n",
    "\n",
    "        # relative improvement check on val_smape\n",
    "        if val_smape + 1e-9 < best_score * (1 - rel_improve):\n",
    "            best_score = val_smape\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        # print progress\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Fold {fold_id} Epoch {epoch:03d} | train_loss: {train_loss:.5f} | val_loss: {val_loss:.5f} | val_smape: {val_smape:.4f}% | best_smape: {best_score:.4f}% | wait: {wait}\")\n",
    "\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping (wait >= {patience}) at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        # cleanup\n",
    "        gc.collect()\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    # save best model state\n",
    "    if best_state is not None:\n",
    "        model_path = os.path.join(model_dir, f\"fold_model_{fold_id}.pth\")\n",
    "        torch.save(best_state, model_path)\n",
    "        print(f\"Saved fold model to {model_path}\")\n",
    "        # load best into model\n",
    "        model.load_state_dict(best_state)\n",
    "    else:\n",
    "        print(\"No improvement recorded; saving last model.\")\n",
    "        model_path = os.path.join(model_dir, f\"fold_model_{fold_id}_last.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# K-Fold training\n",
    "# ---------------------------\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof = np.zeros(train_text_s.shape[0], dtype=np.float32)\n",
    "test_preds = np.zeros(test_text_s.shape[0], dtype=np.float32)\n",
    "fold_smape_list = []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(train_text_s), 1):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Starting Fold {fold}/{N_FOLDS}\")\n",
    "    # training slice\n",
    "    X_t_text = train_text_s[trn_idx]\n",
    "    X_t_img  = train_img_s[trn_idx]\n",
    "    X_t_other= train_other_s[trn_idx]\n",
    "    y_t_log  = y_log[trn_idx]\n",
    "\n",
    "    X_v_text = train_text_s[val_idx]\n",
    "    X_v_img  = train_img_s[val_idx]\n",
    "    X_v_other= train_other_s[val_idx]\n",
    "    y_v_log  = y_log[val_idx]\n",
    "\n",
    "    model = fit_fold(X_t_text, X_t_img, X_t_other, y_t_log,\n",
    "                     X_v_text, X_v_img, X_v_other, y_v_log,\n",
    "                     fold_id=fold, device=DEVICE, model_dir=\"models\",\n",
    "                     epochs=80, batch_size=256, lr=3e-4)\n",
    "\n",
    "    # OOF preds for val set\n",
    "    oof_preds_log = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, X_v_text.shape[0], 512):\n",
    "            t = torch.FloatTensor(X_v_text[i:i+512]).to(DEVICE)\n",
    "            im = torch.FloatTensor(X_v_img[i:i+512]).to(DEVICE)\n",
    "            oth = torch.FloatTensor(X_v_other[i:i+512]).to(DEVICE)\n",
    "            out = model(t, im, oth).cpu().numpy().reshape(-1)\n",
    "            oof_preds_log.append(out)\n",
    "    oof_preds_log = np.concatenate(oof_preds_log)\n",
    "    oof[val_idx] = np.expm1(oof_preds_log)  # store price space\n",
    "\n",
    "    # test preds for fold\n",
    "    fold_test_log = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, test_text_s.shape[0], 512):\n",
    "            t = torch.FloatTensor(test_text_s[i:i+512]).to(DEVICE)\n",
    "            im = torch.FloatTensor(test_img_s[i:i+512]).to(DEVICE)\n",
    "            oth = torch.FloatTensor(test_other_s[i:i+512]).to(DEVICE)\n",
    "            out = model(t, im, oth).cpu().numpy().reshape(-1)\n",
    "            fold_test_log.append(out)\n",
    "    fold_test_log = np.concatenate(fold_test_log)\n",
    "    test_preds += np.expm1(fold_test_log) / N_FOLDS\n",
    "\n",
    "    # fold SMAPE\n",
    "    fold_smape = smape_np(df_train['price'].values[val_idx], oof[val_idx])\n",
    "    fold_smape_list.append(fold_smape)\n",
    "    print(f\"Fold {fold} SMAPE: {fold_smape:.4f}%\")\n",
    "    # cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# ---------------------------\n",
    "# Final evaluation + submission\n",
    "# ---------------------------\n",
    "overall_smape = smape_np(df_train['price'].values, oof)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FOLD SMAPEs:\", fold_smape_list)\n",
    "print(f\"OOF SMAPE: {overall_smape:.4f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save submission\n",
    "final_preds = np.clip(test_preds, 0.01, None)\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': df_test['sample_id'],\n",
    "    'price': final_preds\n",
    "})\n",
    "submission.to_csv(\"enhanced_mlp_fusion_submission_improved.csv\", index=False)\n",
    "print(\"Saved submission -> enhanced_mlp_fusion_submission_improved.csv\")\n",
    "print(\"Prediction stats: min %.2f / mean %.2f / max %.2f\" % (final_preds.min(), final_preds.mean(), final_preds.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eeedb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ OPTIMIZED MLP FUSION: Advanced Price Prediction\n",
      "======================================================================\n",
      "\n",
      "[1/6] Loading data and embeddings...\n",
      "âœ“ Loaded embeddings\n",
      "\n",
      "[2/6] Engineering advanced features...\n",
      "  Extracting advanced features...\n",
      "  Extracting advanced features...\n",
      "âœ“ Enhanced features: 173 dimensions\n",
      "\n",
      "[3/6] Applying optimized scaling...\n",
      "âœ“ Scaling complete\n",
      "\n",
      "[4/6] Training with K-Fold CV...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š FOLD 1/5\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Using device: mps\n",
      "    Epoch 15: train_loss=0.33665, val_loss=0.45577\n",
      "    Epoch 30: train_loss=0.19207, val_loss=0.45364\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 521\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š FOLD \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_FOLDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    519\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mâ”€\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m model = \u001b[43mtrain_optimized_mlp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_text_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_image_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_other_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train_log\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_text_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_image_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_other_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train_log\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# OOF predictions\u001b[39;00m\n\u001b[32m    533\u001b[39m oof_preds[val_idx] = predict_optimized(\n\u001b[32m    534\u001b[39m     model,\n\u001b[32m    535\u001b[39m     train_text_scaled[val_idx],\n\u001b[32m    536\u001b[39m     train_image_scaled[val_idx],\n\u001b[32m    537\u001b[39m     train_other_scaled[val_idx]\n\u001b[32m    538\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 376\u001b[39m, in \u001b[36mtrain_optimized_mlp\u001b[39m\u001b[34m(X_text_tr, X_image_tr, X_other_tr, y_tr, X_text_val, X_image_val, X_other_val, y_val, epochs, batch_size, lr)\u001b[39m\n\u001b[32m    373\u001b[39m     optimizer.step()\n\u001b[32m    374\u001b[39m     scheduler.step()\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m    379\u001b[39m model.eval()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# OPTIMIZED MLP FUSION WITH ADVANCED TECHNIQUES\n",
    "# Target: SMAPE < 40% with Maximum Performance\n",
    "# ===================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ OPTIMIZED MLP FUSION: Advanced Price Prediction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# ENHANCED FEATURE ENGINEERING WITH MORE SIGNALS\n",
    "# ===================================================================\n",
    "def extract_advanced_features(df, is_train=True, price_stats=None):\n",
    "    \"\"\"Extract comprehensive features from all available data\"\"\"\n",
    "    print(f\"  Extracting advanced features...\")\n",
    "   \n",
    "    features = {}\n",
    "   \n",
    "    # ============ TEXT FEATURES ============\n",
    "    catalog = df['catalog_content'].fillna('')\n",
    "   \n",
    "    # Length and structure features\n",
    "    features['title_len'] = catalog.str.len()\n",
    "    features['word_count'] = catalog.str.split().str.len()\n",
    "    features['avg_word_len'] = features['title_len'] / (features['word_count'] + 1)\n",
    "    features['char_count'] = catalog.str.len()\n",
    "   \n",
    "    # Text pattern features\n",
    "    features['has_digits'] = catalog.str.contains(r'\\d', regex=True).astype(int)\n",
    "    features['num_digits'] = catalog.str.count(r'\\d')\n",
    "    features['has_special_chars'] = catalog.str.contains(r'[^a-zA-Z0-9\\s]', regex=True).astype(int)\n",
    "    features['uppercase_ratio'] = catalog.str.count(r'[A-Z]') / (features['title_len'] + 1)\n",
    "    features['digit_ratio'] = features['num_digits'] / (features['title_len'] + 1)\n",
    "   \n",
    "    # Extract numeric values from text\n",
    "    features['num_numbers'] = catalog.str.findall(r'\\d+').str.len().fillna(0)\n",
    "    features['max_number_in_text'] = catalog.str.extractall(r'(\\d+)')[0].astype(float).groupby(level=0).max().reindex(df.index).fillna(0)\n",
    "   \n",
    "    # Price indicators\n",
    "    price_keywords = ['premium', 'luxury', 'pro', 'plus', 'max', 'ultra', 'deluxe', 'professional', 'elite']\n",
    "    budget_keywords = ['basic', 'mini', 'lite', 'eco', 'value', 'budget', 'economy', 'standard']\n",
    "   \n",
    "    features['has_premium_word'] = catalog.str.lower().str.contains('|'.join(price_keywords)).astype(int)\n",
    "    features['has_budget_word'] = catalog.str.lower().str.contains('|'.join(budget_keywords)).astype(int)\n",
    "    features['premium_word_count'] = sum(catalog.str.lower().str.count(kw) for kw in price_keywords)\n",
    "    features['budget_word_count'] = sum(catalog.str.lower().str.count(kw) for kw in budget_keywords)\n",
    "   \n",
    "    # ============ BRAND FEATURES ============\n",
    "    if 'brand' in df.columns:\n",
    "        brand_col = df['brand'].fillna('unknown')\n",
    "       \n",
    "        # Brand frequency\n",
    "        brand_freq = brand_col.value_counts()\n",
    "        features['brand_freq'] = brand_col.map(brand_freq).fillna(0)\n",
    "        features['brand_log_freq'] = np.log1p(features['brand_freq'])\n",
    "       \n",
    "        # Is rare brand\n",
    "        features['is_rare_brand'] = (features['brand_freq'] < 5).astype(int)\n",
    "        features['is_common_brand'] = (features['brand_freq'] > 50).astype(int)\n",
    "       \n",
    "        if is_train and 'price' in df.columns:\n",
    "            # Brand statistics\n",
    "            brand_stats = df.groupby('brand')['price'].agg(['mean', 'std', 'median', 'min', 'max']).to_dict()\n",
    "           \n",
    "            features['brand_mean_price'] = brand_col.map(brand_stats['mean']).fillna(df['price'].median())\n",
    "            features['brand_std_price'] = brand_col.map(brand_stats['std']).fillna(df['price'].std())\n",
    "            features['brand_median_price'] = brand_col.map(brand_stats['median']).fillna(df['price'].median())\n",
    "            features['brand_price_range'] = brand_col.map(lambda x: brand_stats['max'].get(x, 0) - brand_stats['min'].get(x, 0)).fillna(0)\n",
    "           \n",
    "            price_stats_to_return = {\n",
    "                'brand_mean': brand_stats['mean'],\n",
    "                'brand_std': brand_stats['std'],\n",
    "                'brand_median': brand_stats['median'],\n",
    "                'brand_min': brand_stats['min'],\n",
    "                'brand_max': brand_stats['max']\n",
    "            }\n",
    "        elif price_stats is not None:\n",
    "            # Use pre-computed statistics for test set\n",
    "            features['brand_mean_price'] = brand_col.map(price_stats['brand_mean']).fillna(price_stats['global_median'])\n",
    "            features['brand_std_price'] = brand_col.map(price_stats['brand_std']).fillna(price_stats['global_std'])\n",
    "            features['brand_median_price'] = brand_col.map(price_stats['brand_median']).fillna(price_stats['global_median'])\n",
    "            features['brand_price_range'] = brand_col.map(lambda x: price_stats['brand_max'].get(x, 0) - price_stats['brand_min'].get(x, 0)).fillna(0)\n",
    "   \n",
    "    # ============ QUANTITY FEATURES ============\n",
    "    if 'quantity' in df.columns:\n",
    "        qty = df['quantity'].fillna(1)\n",
    "        features['quantity'] = qty\n",
    "        features['log_quantity'] = np.log1p(qty)\n",
    "        features['sqrt_quantity'] = np.sqrt(qty)\n",
    "        features['quantity_squared'] = qty ** 2\n",
    "        features['quantity_cubed'] = qty ** 3\n",
    "        features['inv_quantity'] = 1 / (qty + 0.1)\n",
    "       \n",
    "        # Quantity bins\n",
    "        features['qty_is_one'] = (qty == 1).astype(int)\n",
    "        features['qty_small'] = (qty <= 5).astype(int)\n",
    "        features['qty_medium'] = ((qty > 5) & (qty <= 20)).astype(int)\n",
    "        features['qty_large'] = (qty > 20).astype(int)\n",
    "   \n",
    "    # ============ INTERACTION FEATURES ============\n",
    "    if 'brand_freq' in features and 'word_count' in features:\n",
    "        features['brand_freq_x_words'] = features['brand_freq'] * features['word_count']\n",
    "   \n",
    "    if 'quantity' in features and 'brand_mean_price' in features:\n",
    "        features['qty_x_brand_price'] = features['quantity'] * features['brand_mean_price']\n",
    "   \n",
    "    # ============ POLYNOMIAL FEATURES ============\n",
    "    if 'word_count' in features:\n",
    "        features['word_count_squared'] = features['word_count'] ** 2\n",
    "        features['word_count_log'] = np.log1p(features['word_count'])\n",
    "   \n",
    "    result_df = pd.DataFrame(features)\n",
    "   \n",
    "    if is_train and 'price' in df.columns:\n",
    "        price_stats_to_return = price_stats_to_return if 'brand' in df.columns else {}\n",
    "        price_stats_to_return['global_median'] = df['price'].median()\n",
    "        price_stats_to_return['global_std'] = df['price'].std()\n",
    "        return result_df, price_stats_to_return\n",
    "   \n",
    "    return result_df\n",
    "\n",
    "# ===================================================================\n",
    "# IMPROVED MLP ARCHITECTURE WITH BETTER DESIGN\n",
    "# ===================================================================\n",
    "class OptimizedMultimodalFusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized architecture with:\n",
    "    - Better initialization\n",
    "    - Skip connections\n",
    "    - Improved attention\n",
    "    - Ensemble-ready design\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim, image_dim, other_dim, hidden_dim=768, dropout=0.25):\n",
    "        super().__init__()\n",
    "       \n",
    "        # Modality-specific encoders with skip connections\n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Linear(text_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "       \n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Linear(image_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "       \n",
    "        self.other_encoder = nn.Sequential(\n",
    "            nn.Linear(other_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout * 0.3)\n",
    "        )\n",
    "       \n",
    "        # Cross-modal attention\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim, num_heads=8, dropout=0.1, batch_first=True\n",
    "        )\n",
    "       \n",
    "        # Fusion gate\n",
    "        fusion_input_dim = hidden_dim * 2 + 256\n",
    "        self.fusion_gate = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, fusion_input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "       \n",
    "        # Main fusion pathway with residual connections\n",
    "        self.fusion_block1 = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "       \n",
    "        self.fusion_block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "       \n",
    "        self.fusion_block3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout * 0.3)\n",
    "        )\n",
    "       \n",
    "        self.fusion_block4 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.BatchNorm1d(hidden_dim // 4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "       \n",
    "        self.output_layer = nn.Linear(hidden_dim // 4, 1)\n",
    "       \n",
    "        self._init_weights()\n",
    "   \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.5)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "   \n",
    "    def forward(self, text_emb, image_emb, other_emb):\n",
    "        # Encode modalities\n",
    "        text_enc = self.text_encoder(text_emb)\n",
    "        image_enc = self.image_encoder(image_emb)\n",
    "        other_enc = self.other_encoder(other_emb)\n",
    "       \n",
    "        # Cross-attention between text and image\n",
    "        text_q = text_enc.unsqueeze(1)\n",
    "        image_kv = image_enc.unsqueeze(1)\n",
    "       \n",
    "        text_attended, _ = self.cross_attention(text_q, image_kv, image_kv)\n",
    "        text_attended = text_attended.squeeze(1)\n",
    "       \n",
    "        # Residual connection\n",
    "        text_final = text_enc + 0.3 * text_attended\n",
    "       \n",
    "        # Concatenate all modalities\n",
    "        fused = torch.cat([text_final, image_enc, other_enc], dim=1)\n",
    "       \n",
    "        # Apply gating\n",
    "        gate = self.fusion_gate(fused)\n",
    "        fused = fused * gate\n",
    "       \n",
    "        # Fusion pathway\n",
    "        x = self.fusion_block1(fused)\n",
    "        x = self.fusion_block2(x)\n",
    "        x = self.fusion_block3(x)\n",
    "        x = self.fusion_block4(x)\n",
    "       \n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "# ===================================================================\n",
    "# IMPROVED LOSS FUNCTIONS\n",
    "# ===================================================================\n",
    "def smape_loss(pred, target, epsilon=0.1):\n",
    "    \"\"\"Improved SMAPE loss with better numerical stability\"\"\"\n",
    "    pred_exp = torch.exp(pred)\n",
    "    target_exp = torch.exp(target)\n",
    "   \n",
    "    numerator = torch.abs(pred_exp - target_exp)\n",
    "    denominator = (torch.abs(target_exp) + torch.abs(pred_exp)) / 2.0 + epsilon\n",
    "   \n",
    "    return torch.mean(numerator / denominator)\n",
    "\n",
    "def combined_loss(pred, target, alpha=0.5, beta=0.3):\n",
    "    \"\"\"Multi-component loss for better optimization\"\"\"\n",
    "    # SMAPE component\n",
    "    smape = smape_loss(pred, target)\n",
    "   \n",
    "    # MSE in log space\n",
    "    mse = nn.MSELoss()(pred, target)\n",
    "   \n",
    "    # Huber loss for robustness\n",
    "    huber = nn.SmoothL1Loss()(pred, target)\n",
    "   \n",
    "    return alpha * smape + beta * mse + (1 - alpha - beta) * huber\n",
    "\n",
    "# ===================================================================\n",
    "# OPTIMIZED TRAINING WITH BETTER TECHNIQUES\n",
    "# ===================================================================\n",
    "def train_optimized_mlp(X_text_tr, X_image_tr, X_other_tr, y_tr,\n",
    "                        X_text_val, X_image_val, X_other_val, y_val,\n",
    "                        epochs=200, batch_size=128, lr=8e-5):\n",
    "   \n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print(f\"  Using device: {device}\")\n",
    "   \n",
    "    text_dim = X_text_tr.shape[1]\n",
    "    image_dim = X_image_tr.shape[1]\n",
    "    other_dim = X_other_tr.shape[1]\n",
    "   \n",
    "    model = OptimizedMultimodalFusionMLP(text_dim, image_dim, other_dim).to(device)\n",
    "   \n",
    "    # Better optimizer configuration\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=1e-5,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "   \n",
    "    # Improved scheduler\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr * 10,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(X_text_tr) // batch_size + 1,\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,\n",
    "        final_div_factor=1000.0\n",
    "    )\n",
    "   \n",
    "    # Create datasets\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_text_tr),\n",
    "        torch.FloatTensor(X_image_tr),\n",
    "        torch.FloatTensor(X_other_tr),\n",
    "        torch.FloatTensor(y_tr).unsqueeze(1)\n",
    "    )\n",
    "   \n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_text_val),\n",
    "        torch.FloatTensor(X_image_val),\n",
    "        torch.FloatTensor(X_other_val),\n",
    "        torch.FloatTensor(y_val).unsqueeze(1)\n",
    "    )\n",
    "   \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size * 2, shuffle=False, num_workers=0)\n",
    "   \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 30\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "   \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "       \n",
    "        for text_b, image_b, other_b, y_b in train_loader:\n",
    "            text_b = text_b.to(device)\n",
    "            image_b = image_b.to(device)\n",
    "            other_b = other_b.to(device)\n",
    "            y_b = y_b.to(device)\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            output = model(text_b, image_b, other_b)\n",
    "            loss = combined_loss(output, y_b)\n",
    "           \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "           \n",
    "            train_loss += loss.item()\n",
    "       \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            for text_b, image_b, other_b, y_b in val_loader:\n",
    "                text_b = text_b.to(device)\n",
    "                image_b = image_b.to(device)\n",
    "                other_b = other_b.to(device)\n",
    "                y_b = y_b.to(device)\n",
    "               \n",
    "                output = model(text_b, image_b, other_b)\n",
    "                val_loss += combined_loss(output, y_b).item()\n",
    "       \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "       \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "       \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "       \n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: train_loss={train_loss:.5f}, val_loss={val_loss:.5f}\")\n",
    "   \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# ===================================================================\n",
    "# PREDICTION FUNCTION\n",
    "# ===================================================================\n",
    "def predict_optimized(model, X_text, X_image, X_other, batch_size=512):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_text), batch_size):\n",
    "            end_idx = min(i + batch_size, len(X_text))\n",
    "           \n",
    "            text_b = torch.FloatTensor(X_text[i:end_idx]).to(device)\n",
    "            image_b = torch.FloatTensor(X_image[i:end_idx]).to(device)\n",
    "            other_b = torch.FloatTensor(X_other[i:end_idx]).to(device)\n",
    "           \n",
    "            output = model(text_b, image_b, other_b)\n",
    "            predictions.append(output.cpu().numpy())\n",
    "   \n",
    "    return np.vstack(predictions).flatten()\n",
    "\n",
    "# ===================================================================\n",
    "# MAIN EXECUTION\n",
    "# ===================================================================\n",
    "print(\"\\n[1/6] Loading data and embeddings...\")\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Load embeddings\n",
    "X_train_full = np.load(\"embeddings_medium/final_X_train_medium_with_brand.npy\", allow_pickle=False)\n",
    "X_test_full = np.load(\"embeddings_medium/final_X_test_medium_with_brand.npy\", allow_pickle=False)\n",
    "\n",
    "# Define dimensions\n",
    "text_dim = 384\n",
    "image_dim = 512\n",
    "\n",
    "# Slice embeddings\n",
    "train_text = X_train_full[:, :text_dim]\n",
    "train_image = X_train_full[:, text_dim:text_dim+image_dim]\n",
    "train_other_base = X_train_full[:, text_dim+image_dim:]\n",
    "\n",
    "test_text = X_test_full[:, :text_dim]\n",
    "test_image = X_test_full[:, text_dim:text_dim+image_dim]\n",
    "test_other_base = X_test_full[:, text_dim+image_dim:]\n",
    "\n",
    "print(f\"âœ“ Loaded embeddings\")\n",
    "del X_train_full, X_test_full\n",
    "gc.collect()\n",
    "\n",
    "# ===================================================================\n",
    "# ADVANCED FEATURE ENGINEERING\n",
    "# ===================================================================\n",
    "print(\"\\n[2/6] Engineering advanced features...\")\n",
    "train_extra_features, price_stats = extract_advanced_features(df_train, is_train=True)\n",
    "test_extra_features = extract_advanced_features(df_test, is_train=False, price_stats=price_stats)\n",
    "\n",
    "# Combine features\n",
    "train_other = np.hstack([train_other_base, train_extra_features.values])\n",
    "test_other = np.hstack([test_other_base, test_extra_features.values])\n",
    "\n",
    "print(f\"âœ“ Enhanced features: {train_other.shape[1]} dimensions\")\n",
    "del train_other_base, test_other_base\n",
    "gc.collect()\n",
    "\n",
    "# Target transformation with Box-Cox inspired approach\n",
    "y_train_log = np.log1p(df_train['price'].values)\n",
    "\n",
    "# ===================================================================\n",
    "# OPTIMIZED SCALING STRATEGY\n",
    "# ===================================================================\n",
    "print(\"\\n[3/6] Applying optimized scaling...\")\n",
    "\n",
    "# Use different scalers for different modalities\n",
    "text_scaler = StandardScaler()\n",
    "image_scaler = StandardScaler()\n",
    "other_scaler = RobustScaler()\n",
    "\n",
    "train_text_scaled = text_scaler.fit_transform(train_text)\n",
    "test_text_scaled = text_scaler.transform(test_text)\n",
    "\n",
    "train_image_scaled = image_scaler.fit_transform(train_image)\n",
    "test_image_scaled = image_scaler.transform(test_image)\n",
    "\n",
    "train_other_scaled = other_scaler.fit_transform(train_other)\n",
    "test_other_scaled = other_scaler.transform(test_other)\n",
    "\n",
    "print(\"âœ“ Scaling complete\")\n",
    "del train_text, train_image, train_other, test_text, test_image, test_other\n",
    "gc.collect()\n",
    "\n",
    "# ===================================================================\n",
    "# K-FOLD CROSS-VALIDATION\n",
    "# ===================================================================\n",
    "print(\"\\n[4/6] Training with K-Fold CV...\")\n",
    "\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = np.zeros(len(train_text_scaled))\n",
    "test_preds = np.zeros(len(test_text_scaled))\n",
    "\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_text_scaled), 1):\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"ðŸ“Š FOLD {fold}/{N_FOLDS}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "   \n",
    "    model = train_optimized_mlp(\n",
    "        train_text_scaled[train_idx],\n",
    "        train_image_scaled[train_idx],\n",
    "        train_other_scaled[train_idx],\n",
    "        y_train_log[train_idx],\n",
    "        train_text_scaled[val_idx],\n",
    "        train_image_scaled[val_idx],\n",
    "        train_other_scaled[val_idx],\n",
    "        y_train_log[val_idx]\n",
    "    )\n",
    "   \n",
    "    # OOF predictions\n",
    "    oof_preds[val_idx] = predict_optimized(\n",
    "        model,\n",
    "        train_text_scaled[val_idx],\n",
    "        train_image_scaled[val_idx],\n",
    "        train_other_scaled[val_idx]\n",
    "    )\n",
    "   \n",
    "    # Test predictions\n",
    "    fold_test_preds = predict_optimized(\n",
    "        model,\n",
    "        test_text_scaled,\n",
    "        test_image_scaled,\n",
    "        test_other_scaled\n",
    "    )\n",
    "    test_preds += fold_test_preds / N_FOLDS\n",
    "   \n",
    "    # Calculate fold SMAPE\n",
    "    val_pred_price = np.expm1(oof_preds[val_idx])\n",
    "    val_actual_price = np.expm1(y_train_log[val_idx])\n",
    "   \n",
    "    fold_smape = np.mean(\n",
    "        2 * np.abs(val_pred_price - val_actual_price) /\n",
    "        (np.abs(val_actual_price) + np.abs(val_pred_price) + 1e-8)\n",
    "    ) * 100\n",
    "   \n",
    "    fold_scores.append(fold_smape)\n",
    "    print(f\"  ðŸ“ˆ Fold {fold} SMAPE: {fold_smape:.4f}%\")\n",
    "   \n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# ===================================================================\n",
    "# POST-PROCESSING AND CALIBRATION\n",
    "# ===================================================================\n",
    "print(\"\\n[5/6] Applying post-processing...\")\n",
    "\n",
    "# Calibrate predictions using validation set\n",
    "oof_prices_raw = np.expm1(oof_preds)\n",
    "actual_prices = df_train['price'].values\n",
    "\n",
    "# Simple bias correction\n",
    "bias = np.median(actual_prices / (oof_prices_raw + 0.01))\n",
    "test_preds_calibrated = test_preds + np.log(bias)\n",
    "\n",
    "# ===================================================================\n",
    "# FINAL EVALUATION\n",
    "# ===================================================================\n",
    "print(\"\\n[6/6] Final evaluation and submission...\")\n",
    "\n",
    "overall_smape = np.mean(\n",
    "    2 * np.abs(oof_prices_raw - actual_prices) /\n",
    "    (np.abs(actual_prices) + np.abs(oof_prices_raw) + 1e-8)\n",
    ") * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸ“Š CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for i, score in enumerate(fold_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f}%\")\n",
    "print(f\"\\n  Mean: {np.mean(fold_scores):.4f}%\")\n",
    "print(f\"  Std:  {np.std(fold_scores):.4f}%\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸŽ¯ FINAL OOF SMAPE: {overall_smape:.4f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# CREATE SUBMISSION\n",
    "# ===================================================================\n",
    "final_predictions = np.expm1(test_preds_calibrated)\n",
    "final_predictions = np.clip(final_predictions, 0.01, None)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': df_test['sample_id'],\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('optimized_mlp_fusion_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Submission saved: optimized_mlp_fusion_submission.csv\")\n",
    "print(\"\\nðŸ“‹ Prediction statistics:\")\n",
    "print(f\"  Min:    ${final_predictions.min():.2f}\")\n",
    "print(f\"  Max:    ${final_predictions.max():.2f}\")\n",
    "print(f\"  Mean:   ${final_predictions.mean():.2f}\")\n",
    "print(f\"  Median: ${np.median(final_predictions):.2f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d289aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED MLP FUSION WITH ADVANCED FEATURE ENGINEERING\n",
    "# Target: SMAPE < 45% with Maximum Data Extraction\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ ENHANCED MLP FUSION: Maximum Feature Extraction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# ADVANCED FEATURE ENGINEERING\n",
    "# ===================================================================\n",
    "def extract_advanced_features(df, is_train=True):\n",
    "    \"\"\"Extract rich features from catalog_content and other columns\"\"\"\n",
    "    print(f\"  Extracting advanced features...\")\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Text length features\n",
    "    features['title_len'] = df['catalog_content'].str.len()\n",
    "    features['word_count'] = df['catalog_content'].str.split().str.len()\n",
    "    features['avg_word_len'] = features['title_len'] / (features['word_count'] + 1)\n",
    "    \n",
    "    # Brand features (if present)\n",
    "    if 'brand' in df.columns:\n",
    "        # Brand frequency encoding\n",
    "        brand_freq = df['brand'].value_counts()\n",
    "        features['brand_freq'] = df['brand'].map(brand_freq).fillna(0)\n",
    "        \n",
    "        # Brand mean price (only for train)\n",
    "        if is_train and 'price' in df.columns:\n",
    "            brand_mean = df.groupby('brand')['price'].mean()\n",
    "            features['brand_mean_price'] = df['brand'].map(brand_mean).fillna(df['price'].median())\n",
    "    \n",
    "    # Quantity features\n",
    "    if 'quantity' in df.columns:\n",
    "        features['quantity'] = df['quantity'].fillna(1)\n",
    "        features['log_quantity'] = np.log1p(features['quantity'])\n",
    "        features['quantity_squared'] = features['quantity'] ** 2\n",
    "    \n",
    "    # Text pattern features\n",
    "    features['has_digits'] = df['catalog_content'].str.contains(r'\\d', regex=True).astype(int)\n",
    "    features['has_special_chars'] = df['catalog_content'].str.contains(r'[^a-zA-Z0-9\\s]', regex=True).astype(int)\n",
    "    features['uppercase_ratio'] = df['catalog_content'].str.count(r'[A-Z]') / (features['title_len'] + 1)\n",
    "    \n",
    "    # Extract numeric values from text\n",
    "    features['num_numbers'] = df['catalog_content'].str.findall(r'\\d+').str.len().fillna(0)\n",
    "    \n",
    "    # Check for common price indicators\n",
    "    price_keywords = ['premium', 'luxury', 'pro', 'plus', 'max', 'ultra', 'deluxe']\n",
    "    budget_keywords = ['basic', 'mini', 'lite', 'eco', 'value']\n",
    "    \n",
    "    features['has_premium_word'] = df['catalog_content'].str.lower().str.contains('|'.join(price_keywords)).astype(int)\n",
    "    features['has_budget_word'] = df['catalog_content'].str.lower().str.contains('|'.join(budget_keywords)).astype(int)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# ENHANCED MLP ARCHITECTURE WITH RESIDUAL CONNECTIONS\n",
    "\n",
    "class EnhancedMultimodalFusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced fusion with:\n",
    "    - Residual connections\n",
    "    - Better attention mechanism\n",
    "    - Gated fusion\n",
    "    - More sophisticated architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim, image_dim, other_dim, hidden_dim=768, dropout=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Individual encoders with residual capability\n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Linear(text_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.7)\n",
    "        )\n",
    "        \n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Linear(image_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.7)\n",
    "        )\n",
    "        \n",
    "        self.other_encoder = nn.Sequential(\n",
    "            nn.Linear(other_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Cross-attention between modalities\n",
    "        self.text_to_image_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim, num_heads=8, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.image_to_text_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim, num_heads=8, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Gated fusion mechanism\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + 128, hidden_dim * 2),  # Changed from hidden_dim to hidden_dim * 2\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Main fusion network with residual connections\n",
    "        fusion_input_dim = hidden_dim * 2 + 128\n",
    "        self.fusion1 = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.fusion2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.7)\n",
    "        )\n",
    "        \n",
    "        self.fusion3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Linear(hidden_dim // 2, 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, text_emb, image_emb, other_emb):\n",
    "        # Encode each modality\n",
    "        text_enc = self.text_encoder(text_emb)\n",
    "        image_enc = self.image_encoder(image_emb)\n",
    "        other_enc = self.other_encoder(other_emb)\n",
    "        \n",
    "        # Bidirectional cross-attention\n",
    "        text_att, _ = self.text_to_image_attn(\n",
    "            text_enc.unsqueeze(1), \n",
    "            image_enc.unsqueeze(1), \n",
    "            image_enc.unsqueeze(1)\n",
    "        )\n",
    "        text_att = text_att.squeeze(1)\n",
    "        \n",
    "        image_att, _ = self.image_to_text_attn(\n",
    "            image_enc.unsqueeze(1), \n",
    "            text_enc.unsqueeze(1), \n",
    "            text_enc.unsqueeze(1)\n",
    "        )\n",
    "        image_att = image_att.squeeze(1)\n",
    "        \n",
    "        # Combine with residuals\n",
    "        text_combined = text_enc + text_att\n",
    "        image_combined = image_enc + image_att\n",
    "        \n",
    "        # Concatenate all features\n",
    "        fused = torch.cat([text_combined, image_combined, other_enc], dim=1)\n",
    "        \n",
    "        # Gated fusion\n",
    "        gate_values = self.gate(fused)\n",
    "        \n",
    "        # Apply fusion layers with residuals\n",
    "        x = self.fusion1(fused)\n",
    "        x = x * gate_values[:, :x.size(1)]  # Apply gating\n",
    "        \n",
    "        x = self.fusion2(x)\n",
    "        x = self.fusion3(x)\n",
    "        output = self.output(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# CUSTOM LOSS FUNCTION - SMAPE-inspired\n",
    "def smape_loss(pred, target, epsilon=1e-3):\n",
    "    \"\"\"SMAPE-inspired loss that directly optimizes the evaluation metric\"\"\"\n",
    "    # Work in log space to match our target\n",
    "    pred_exp = torch.exp(pred)\n",
    "    target_exp = torch.exp(target)\n",
    "    \n",
    "    numerator = torch.abs(pred_exp - target_exp)\n",
    "    denominator = (torch.abs(target_exp) + torch.abs(pred_exp)) / 2.0 + epsilon\n",
    "    \n",
    "    return torch.mean(numerator / denominator)\n",
    "\n",
    "def combined_loss(pred, target, alpha=0.5):\n",
    "    \"\"\"Combine SMAPE loss with Huber loss for stability\"\"\"\n",
    "    smape = smape_loss(pred, target)\n",
    "    huber = nn.SmoothL1Loss()(pred, target)\n",
    "    return alpha * smape + (1 - alpha) * huber\n",
    "\n",
    "# ===================================================================\n",
    "# TRAINING FUNCTION WITH ADVANCED TECHNIQUES\n",
    "# ===================================================================\n",
    "def train_enhanced_mlp(X_text_tr, X_image_tr, X_other_tr, y_tr, \n",
    "                       X_text_val, X_image_val, X_other_val, y_val,\n",
    "                       epochs=150, batch_size=256, lr=3e-4):\n",
    "    \n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print(f\"  Using device: {device}\")\n",
    "    \n",
    "    text_dim = X_text_tr.shape[1]\n",
    "    image_dim = X_image_tr.shape[1]\n",
    "    other_dim = X_other_tr.shape[1]\n",
    "    \n",
    "    model = EnhancedMultimodalFusionMLP(text_dim, image_dim, other_dim).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-5, betas=(0.9, 0.999))\n",
    "    \n",
    "    # Cosine annealing with warm restarts\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=15, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_text_tr),\n",
    "        torch.FloatTensor(X_image_tr),\n",
    "        torch.FloatTensor(X_other_tr),\n",
    "        torch.FloatTensor(y_tr).unsqueeze(1)\n",
    "    )\n",
    "    \n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_text_val),\n",
    "        torch.FloatTensor(X_image_val),\n",
    "        torch.FloatTensor(X_other_val),\n",
    "        torch.FloatTensor(y_val).unsqueeze(1)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for text_b, image_b, other_b, y_b in train_loader:\n",
    "            text_b = text_b.to(device)\n",
    "            image_b = image_b.to(device)\n",
    "            other_b = other_b.to(device)\n",
    "            y_b = y_b.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(text_b, image_b, other_b)\n",
    "            loss = combined_loss(output, y_b)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text_b, image_b, other_b, y_b in val_loader:\n",
    "                text_b = text_b.to(device)\n",
    "                image_b = image_b.to(device)\n",
    "                other_b = other_b.to(device)\n",
    "                y_b = y_b.to(device)\n",
    "                \n",
    "                output = model(text_b, image_b, other_b)\n",
    "                val_loss += combined_loss(output, y_b).item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"    Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: train_loss={train_loss:.5f}, val_loss={val_loss:.5f}\")\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# ===================================================================\n",
    "# PREDICTION FUNCTION\n",
    "# ===================================================================\n",
    "def predict_enhanced(model, X_text, X_image, X_other, batch_size=512):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(X_text), batch_size), desc=\"Predicting\", leave=False):\n",
    "            end_idx = min(i + batch_size, len(X_text))\n",
    "            \n",
    "            text_b = torch.FloatTensor(X_text[i:end_idx]).to(device)\n",
    "            image_b = torch.FloatTensor(X_image[i:end_idx]).to(device)\n",
    "            other_b = torch.FloatTensor(X_other[i:end_idx]).to(device)\n",
    "            \n",
    "            output = model(text_b, image_b, other_b)\n",
    "            predictions.append(output.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(predictions).flatten()\n",
    "\n",
    "# ===================================================================\n",
    "# MAIN EXECUTION\n",
    "# ===================================================================\n",
    "print(\"\\n[1/5] Loading data and embeddings...\")\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Load embeddings\n",
    "X_train_full = np.load(\"final_X_train_medium_with_brand.npy\", allow_pickle=False)\n",
    "X_test_full = np.load(\"final_X_test_medium_with_brand.npy\", allow_pickle=False)\n",
    "\n",
    "# Define dimensions\n",
    "text_dim = 384\n",
    "image_dim = 512\n",
    "\n",
    "# Slice embeddings\n",
    "train_text = X_train_full[:, :text_dim]\n",
    "train_image = X_train_full[:, text_dim:text_dim+image_dim]\n",
    "train_other_base = X_train_full[:, text_dim+image_dim:]\n",
    "\n",
    "test_text = X_test_full[:, :text_dim]\n",
    "test_image = X_test_full[:, text_dim:text_dim+image_dim]\n",
    "test_other_base = X_test_full[:, text_dim+image_dim:]\n",
    "\n",
    "print(f\"âœ“ Loaded embeddings\")\n",
    "del X_train_full, X_test_full\n",
    "gc.collect()\n",
    "\n",
    "# ===================================================================\n",
    "# ADVANCED FEATURE ENGINEERING\n",
    "# ===================================================================\n",
    "print(\"\\n[2/5] Engineering advanced features...\")\n",
    "train_extra_features = extract_advanced_features(df_train, is_train=True)\n",
    "test_extra_features = extract_advanced_features(df_test, is_train=False)\n",
    "\n",
    "# Combine with existing other features\n",
    "train_other = np.hstack([train_other_base, train_extra_features.values])\n",
    "test_other = np.hstack([test_other_base, test_extra_features.values])\n",
    "\n",
    "print(f\"âœ“ Enhanced features: {train_other.shape[1]} dimensions\")\n",
    "del train_other_base, test_other_base\n",
    "gc.collect()\n",
    "\n",
    "# Target transformation\n",
    "y_train_log = np.log1p(df_train['price'].values)\n",
    "\n",
    "# ===================================================================\n",
    "# ADVANCED SCALING\n",
    "# ===================================================================\n",
    "print(\"\\n[3/5] Applying robust scaling...\")\n",
    "\n",
    "# Use QuantileTransformer for embeddings (more robust to outliers)\n",
    "text_scaler = QuantileTransformer(n_quantiles=1000, output_distribution='normal')\n",
    "image_scaler = QuantileTransformer(n_quantiles=1000, output_distribution='normal')\n",
    "other_scaler = RobustScaler()\n",
    "\n",
    "train_text_scaled = text_scaler.fit_transform(train_text)\n",
    "test_text_scaled = text_scaler.transform(test_text)\n",
    "\n",
    "train_image_scaled = image_scaler.fit_transform(train_image)\n",
    "test_image_scaled = image_scaler.transform(test_image)\n",
    "\n",
    "train_other_scaled = other_scaler.fit_transform(train_other)\n",
    "test_other_scaled = other_scaler.transform(test_other)\n",
    "\n",
    "print(\"âœ“ Scaling complete\")\n",
    "del train_text, train_image, train_other, test_text, test_image, test_other\n",
    "gc.collect()\n",
    "\n",
    "# ===================================================================\n",
    "# K-FOLD CROSS-VALIDATION WITH STRATIFICATION\n",
    "# ===================================================================\n",
    "print(\"\\n[4/5] Training with K-Fold CV...\")\n",
    "\n",
    "N_FOLDS = 7  # Increased from 5 to 7 for better generalization\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = np.zeros(len(train_text_scaled))\n",
    "test_preds = np.zeros(len(test_text_scaled))\n",
    "\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_text_scaled), 1):\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"ðŸ“Š FOLD {fold}/{N_FOLDS}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    \n",
    "    model = train_enhanced_mlp(\n",
    "        train_text_scaled[train_idx], \n",
    "        train_image_scaled[train_idx], \n",
    "        train_other_scaled[train_idx], \n",
    "        y_train_log[train_idx],\n",
    "        train_text_scaled[val_idx], \n",
    "        train_image_scaled[val_idx], \n",
    "        train_other_scaled[val_idx], \n",
    "        y_train_log[val_idx]\n",
    "    )\n",
    "    \n",
    "    # OOF predictions\n",
    "    oof_preds[val_idx] = predict_enhanced(\n",
    "        model, \n",
    "        train_text_scaled[val_idx], \n",
    "        train_image_scaled[val_idx], \n",
    "        train_other_scaled[val_idx]\n",
    "    )\n",
    "    \n",
    "    # Test predictions\n",
    "    fold_test_preds = predict_enhanced(\n",
    "        model, \n",
    "        test_text_scaled, \n",
    "        test_image_scaled, \n",
    "        test_other_scaled\n",
    "    )\n",
    "    test_preds += fold_test_preds / N_FOLDS\n",
    "    \n",
    "    # Calculate fold SMAPE\n",
    "    val_pred_price = np.expm1(oof_preds[val_idx])\n",
    "    val_actual_price = np.expm1(y_train_log[val_idx])\n",
    "    \n",
    "    fold_smape = np.mean(\n",
    "        2 * np.abs(val_pred_price - val_actual_price) / \n",
    "        (np.abs(val_actual_price) + np.abs(val_pred_price) + 1e-8)\n",
    "    ) * 100\n",
    "    \n",
    "    fold_scores.append(fold_smape)\n",
    "    print(f\"  ðŸ“ˆ Fold {fold} SMAPE: {fold_smape:.4f}%\")\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# ===================================================================\n",
    "# FINAL EVALUATION\n",
    "# ===================================================================\n",
    "print(\"\\n[5/5] Final evaluation and submission...\")\n",
    "\n",
    "oof_prices = np.expm1(oof_preds)\n",
    "actual_prices = df_train['price'].values\n",
    "\n",
    "overall_smape = np.mean(\n",
    "    2 * np.abs(oof_prices - actual_prices) / \n",
    "    (np.abs(actual_prices) + np.abs(oof_prices) + 1e-8)\n",
    ") * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸ“Š CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for i, score in enumerate(fold_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f}%\")\n",
    "print(f\"\\n  Mean: {np.mean(fold_scores):.4f}%\")\n",
    "print(f\"  Std:  {np.std(fold_scores):.4f}%\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸŽ¯ FINAL OOF SMAPE: {overall_smape:.4f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# CREATE SUBMISSION\n",
    "# ===================================================================\n",
    "final_predictions = np.expm1(test_preds)\n",
    "final_predictions = np.clip(final_predictions, 0.01, None)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': df_test['sample_id'],\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('enhanced_mlp_fusion_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Submission saved: enhanced_mlp_fusion_submission.csv\")\n",
    "print(\"\\nðŸ“‹ Prediction statistics:\")\n",
    "print(f\"  Min:    ${final_predictions.min():.2f}\")\n",
    "print(f\"  Max:    ${final_predictions.max():.2f}\")\n",
    "print(f\"  Mean:   ${final_predictions.mean():.2f}\")\n",
    "print(f\"  Median: ${np.median(final_predictions):.2f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
